{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 687,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001455604075691412,
      "grad_norm": 2.161638021469116,
      "learning_rate": 0.0,
      "loss": 2.4357,
      "step": 1
    },
    {
      "epoch": 0.002911208151382824,
      "grad_norm": 2.097327470779419,
      "learning_rate": 4e-05,
      "loss": 2.3201,
      "step": 2
    },
    {
      "epoch": 0.004366812227074236,
      "grad_norm": 2.02821683883667,
      "learning_rate": 8e-05,
      "loss": 2.5402,
      "step": 3
    },
    {
      "epoch": 0.005822416302765648,
      "grad_norm": 2.038220167160034,
      "learning_rate": 0.00012,
      "loss": 2.2147,
      "step": 4
    },
    {
      "epoch": 0.00727802037845706,
      "grad_norm": 1.568902850151062,
      "learning_rate": 0.00016,
      "loss": 2.1839,
      "step": 5
    },
    {
      "epoch": 0.008733624454148471,
      "grad_norm": 1.0990835428237915,
      "learning_rate": 0.0002,
      "loss": 1.8118,
      "step": 6
    },
    {
      "epoch": 0.010189228529839884,
      "grad_norm": 1.183762550354004,
      "learning_rate": 0.0001997067448680352,
      "loss": 1.6416,
      "step": 7
    },
    {
      "epoch": 0.011644832605531296,
      "grad_norm": 1.0520267486572266,
      "learning_rate": 0.0001994134897360704,
      "loss": 1.7428,
      "step": 8
    },
    {
      "epoch": 0.013100436681222707,
      "grad_norm": 0.8312734961509705,
      "learning_rate": 0.00019912023460410557,
      "loss": 1.4541,
      "step": 9
    },
    {
      "epoch": 0.01455604075691412,
      "grad_norm": 1.0482845306396484,
      "learning_rate": 0.00019882697947214077,
      "loss": 1.2512,
      "step": 10
    },
    {
      "epoch": 0.01601164483260553,
      "grad_norm": 1.1685489416122437,
      "learning_rate": 0.00019853372434017595,
      "loss": 1.0496,
      "step": 11
    },
    {
      "epoch": 0.017467248908296942,
      "grad_norm": 1.0905449390411377,
      "learning_rate": 0.00019824046920821116,
      "loss": 1.1046,
      "step": 12
    },
    {
      "epoch": 0.018922852983988356,
      "grad_norm": 1.046441912651062,
      "learning_rate": 0.00019794721407624633,
      "loss": 0.9473,
      "step": 13
    },
    {
      "epoch": 0.020378457059679767,
      "grad_norm": 0.8421619534492493,
      "learning_rate": 0.00019765395894428154,
      "loss": 0.9364,
      "step": 14
    },
    {
      "epoch": 0.021834061135371178,
      "grad_norm": 0.6297779679298401,
      "learning_rate": 0.00019736070381231672,
      "loss": 1.0186,
      "step": 15
    },
    {
      "epoch": 0.023289665211062592,
      "grad_norm": 0.4688524305820465,
      "learning_rate": 0.00019706744868035192,
      "loss": 0.9037,
      "step": 16
    },
    {
      "epoch": 0.024745269286754003,
      "grad_norm": 0.5828601121902466,
      "learning_rate": 0.0001967741935483871,
      "loss": 1.019,
      "step": 17
    },
    {
      "epoch": 0.026200873362445413,
      "grad_norm": 0.5727140307426453,
      "learning_rate": 0.0001964809384164223,
      "loss": 1.0074,
      "step": 18
    },
    {
      "epoch": 0.027656477438136828,
      "grad_norm": 0.6348596215248108,
      "learning_rate": 0.00019618768328445748,
      "loss": 1.0023,
      "step": 19
    },
    {
      "epoch": 0.02911208151382824,
      "grad_norm": 0.7030735611915588,
      "learning_rate": 0.00019589442815249269,
      "loss": 0.8028,
      "step": 20
    },
    {
      "epoch": 0.03056768558951965,
      "grad_norm": 0.5602253675460815,
      "learning_rate": 0.00019560117302052786,
      "loss": 1.1202,
      "step": 21
    },
    {
      "epoch": 0.03202328966521106,
      "grad_norm": 0.5448229312896729,
      "learning_rate": 0.00019530791788856307,
      "loss": 0.9571,
      "step": 22
    },
    {
      "epoch": 0.033478893740902474,
      "grad_norm": 0.5888112187385559,
      "learning_rate": 0.00019501466275659825,
      "loss": 0.9251,
      "step": 23
    },
    {
      "epoch": 0.034934497816593885,
      "grad_norm": 0.4937477111816406,
      "learning_rate": 0.00019472140762463345,
      "loss": 0.9503,
      "step": 24
    },
    {
      "epoch": 0.036390101892285295,
      "grad_norm": 0.5724827647209167,
      "learning_rate": 0.00019442815249266863,
      "loss": 1.1119,
      "step": 25
    },
    {
      "epoch": 0.03784570596797671,
      "grad_norm": 0.5211819410324097,
      "learning_rate": 0.00019413489736070383,
      "loss": 0.9867,
      "step": 26
    },
    {
      "epoch": 0.039301310043668124,
      "grad_norm": 0.47337308526039124,
      "learning_rate": 0.000193841642228739,
      "loss": 0.6836,
      "step": 27
    },
    {
      "epoch": 0.040756914119359534,
      "grad_norm": 0.5586395263671875,
      "learning_rate": 0.00019354838709677422,
      "loss": 0.7959,
      "step": 28
    },
    {
      "epoch": 0.042212518195050945,
      "grad_norm": 0.4464186728000641,
      "learning_rate": 0.0001932551319648094,
      "loss": 0.6163,
      "step": 29
    },
    {
      "epoch": 0.043668122270742356,
      "grad_norm": 0.5250752568244934,
      "learning_rate": 0.0001929618768328446,
      "loss": 0.7443,
      "step": 30
    },
    {
      "epoch": 0.04512372634643377,
      "grad_norm": 0.461708128452301,
      "learning_rate": 0.00019266862170087977,
      "loss": 0.9617,
      "step": 31
    },
    {
      "epoch": 0.046579330422125184,
      "grad_norm": 0.4636455476284027,
      "learning_rate": 0.00019237536656891498,
      "loss": 0.6237,
      "step": 32
    },
    {
      "epoch": 0.048034934497816595,
      "grad_norm": 0.47957995533943176,
      "learning_rate": 0.00019208211143695016,
      "loss": 0.9456,
      "step": 33
    },
    {
      "epoch": 0.049490538573508006,
      "grad_norm": 0.45596712827682495,
      "learning_rate": 0.00019178885630498536,
      "loss": 0.8887,
      "step": 34
    },
    {
      "epoch": 0.050946142649199416,
      "grad_norm": 0.41715341806411743,
      "learning_rate": 0.00019149560117302054,
      "loss": 0.8481,
      "step": 35
    },
    {
      "epoch": 0.05240174672489083,
      "grad_norm": 0.41256603598594666,
      "learning_rate": 0.00019120234604105574,
      "loss": 0.7481,
      "step": 36
    },
    {
      "epoch": 0.053857350800582245,
      "grad_norm": 0.5070394277572632,
      "learning_rate": 0.00019090909090909092,
      "loss": 0.9105,
      "step": 37
    },
    {
      "epoch": 0.055312954876273655,
      "grad_norm": 0.44884783029556274,
      "learning_rate": 0.00019061583577712613,
      "loss": 0.7661,
      "step": 38
    },
    {
      "epoch": 0.056768558951965066,
      "grad_norm": 0.4856427013874054,
      "learning_rate": 0.0001903225806451613,
      "loss": 0.8771,
      "step": 39
    },
    {
      "epoch": 0.05822416302765648,
      "grad_norm": 0.4620927572250366,
      "learning_rate": 0.0001900293255131965,
      "loss": 0.6989,
      "step": 40
    },
    {
      "epoch": 0.05967976710334789,
      "grad_norm": 0.4387282729148865,
      "learning_rate": 0.0001897360703812317,
      "loss": 0.7979,
      "step": 41
    },
    {
      "epoch": 0.0611353711790393,
      "grad_norm": 0.482201486825943,
      "learning_rate": 0.0001894428152492669,
      "loss": 0.9216,
      "step": 42
    },
    {
      "epoch": 0.06259097525473072,
      "grad_norm": 0.4375556707382202,
      "learning_rate": 0.00018914956011730207,
      "loss": 0.6812,
      "step": 43
    },
    {
      "epoch": 0.06404657933042213,
      "grad_norm": 0.42401188611984253,
      "learning_rate": 0.00018885630498533727,
      "loss": 0.6848,
      "step": 44
    },
    {
      "epoch": 0.06550218340611354,
      "grad_norm": 0.4282740354537964,
      "learning_rate": 0.00018856304985337245,
      "loss": 0.858,
      "step": 45
    },
    {
      "epoch": 0.06695778748180495,
      "grad_norm": 0.5870473384857178,
      "learning_rate": 0.00018826979472140766,
      "loss": 0.9264,
      "step": 46
    },
    {
      "epoch": 0.06841339155749636,
      "grad_norm": 0.47352930903434753,
      "learning_rate": 0.00018797653958944283,
      "loss": 0.846,
      "step": 47
    },
    {
      "epoch": 0.06986899563318777,
      "grad_norm": 0.44286271929740906,
      "learning_rate": 0.00018768328445747804,
      "loss": 1.1047,
      "step": 48
    },
    {
      "epoch": 0.07132459970887918,
      "grad_norm": 0.4769907295703888,
      "learning_rate": 0.0001873900293255132,
      "loss": 0.7813,
      "step": 49
    },
    {
      "epoch": 0.07278020378457059,
      "grad_norm": 0.43778491020202637,
      "learning_rate": 0.0001870967741935484,
      "loss": 0.9706,
      "step": 50
    },
    {
      "epoch": 0.07423580786026202,
      "grad_norm": 0.4446662366390228,
      "learning_rate": 0.00018680351906158357,
      "loss": 0.8054,
      "step": 51
    },
    {
      "epoch": 0.07569141193595343,
      "grad_norm": 0.5098065137863159,
      "learning_rate": 0.00018651026392961878,
      "loss": 0.9643,
      "step": 52
    },
    {
      "epoch": 0.07714701601164484,
      "grad_norm": 0.5207013487815857,
      "learning_rate": 0.00018621700879765395,
      "loss": 0.6634,
      "step": 53
    },
    {
      "epoch": 0.07860262008733625,
      "grad_norm": 0.4249691665172577,
      "learning_rate": 0.00018592375366568916,
      "loss": 0.8589,
      "step": 54
    },
    {
      "epoch": 0.08005822416302766,
      "grad_norm": 0.4153844714164734,
      "learning_rate": 0.00018563049853372434,
      "loss": 0.8546,
      "step": 55
    },
    {
      "epoch": 0.08151382823871907,
      "grad_norm": 0.4267479181289673,
      "learning_rate": 0.00018533724340175954,
      "loss": 1.0077,
      "step": 56
    },
    {
      "epoch": 0.08296943231441048,
      "grad_norm": 0.4971555769443512,
      "learning_rate": 0.00018504398826979472,
      "loss": 1.0451,
      "step": 57
    },
    {
      "epoch": 0.08442503639010189,
      "grad_norm": 0.503242552280426,
      "learning_rate": 0.00018475073313782992,
      "loss": 0.8859,
      "step": 58
    },
    {
      "epoch": 0.0858806404657933,
      "grad_norm": 0.40034735202789307,
      "learning_rate": 0.0001844574780058651,
      "loss": 0.712,
      "step": 59
    },
    {
      "epoch": 0.08733624454148471,
      "grad_norm": 0.4366995394229889,
      "learning_rate": 0.0001841642228739003,
      "loss": 0.8475,
      "step": 60
    },
    {
      "epoch": 0.08879184861717612,
      "grad_norm": 0.45377957820892334,
      "learning_rate": 0.00018387096774193548,
      "loss": 1.0249,
      "step": 61
    },
    {
      "epoch": 0.09024745269286755,
      "grad_norm": 0.424405962228775,
      "learning_rate": 0.0001835777126099707,
      "loss": 0.9202,
      "step": 62
    },
    {
      "epoch": 0.09170305676855896,
      "grad_norm": 0.4151865541934967,
      "learning_rate": 0.00018328445747800586,
      "loss": 1.0524,
      "step": 63
    },
    {
      "epoch": 0.09315866084425037,
      "grad_norm": 0.3941720426082611,
      "learning_rate": 0.00018299120234604107,
      "loss": 0.7325,
      "step": 64
    },
    {
      "epoch": 0.09461426491994178,
      "grad_norm": 0.3998282849788666,
      "learning_rate": 0.00018269794721407625,
      "loss": 0.5869,
      "step": 65
    },
    {
      "epoch": 0.09606986899563319,
      "grad_norm": 0.4112178683280945,
      "learning_rate": 0.00018240469208211145,
      "loss": 0.8217,
      "step": 66
    },
    {
      "epoch": 0.0975254730713246,
      "grad_norm": 0.416836142539978,
      "learning_rate": 0.00018211143695014663,
      "loss": 0.7761,
      "step": 67
    },
    {
      "epoch": 0.09898107714701601,
      "grad_norm": 0.4585410952568054,
      "learning_rate": 0.00018181818181818183,
      "loss": 0.6961,
      "step": 68
    },
    {
      "epoch": 0.10043668122270742,
      "grad_norm": 0.4543675184249878,
      "learning_rate": 0.000181524926686217,
      "loss": 0.724,
      "step": 69
    },
    {
      "epoch": 0.10189228529839883,
      "grad_norm": 0.42610806226730347,
      "learning_rate": 0.0001812316715542522,
      "loss": 0.9907,
      "step": 70
    },
    {
      "epoch": 0.10334788937409024,
      "grad_norm": 0.4524792432785034,
      "learning_rate": 0.0001809384164222874,
      "loss": 0.9766,
      "step": 71
    },
    {
      "epoch": 0.10480349344978165,
      "grad_norm": 0.42774730920791626,
      "learning_rate": 0.00018064516129032257,
      "loss": 0.7488,
      "step": 72
    },
    {
      "epoch": 0.10625909752547306,
      "grad_norm": 0.4520675539970398,
      "learning_rate": 0.00018035190615835778,
      "loss": 0.7176,
      "step": 73
    },
    {
      "epoch": 0.10771470160116449,
      "grad_norm": 0.44942936301231384,
      "learning_rate": 0.00018005865102639295,
      "loss": 1.0034,
      "step": 74
    },
    {
      "epoch": 0.1091703056768559,
      "grad_norm": 0.4431282579898834,
      "learning_rate": 0.00017976539589442816,
      "loss": 0.8666,
      "step": 75
    },
    {
      "epoch": 0.11062590975254731,
      "grad_norm": 0.45340731739997864,
      "learning_rate": 0.00017947214076246334,
      "loss": 0.7729,
      "step": 76
    },
    {
      "epoch": 0.11208151382823872,
      "grad_norm": 0.4698356091976166,
      "learning_rate": 0.00017917888563049854,
      "loss": 1.0555,
      "step": 77
    },
    {
      "epoch": 0.11353711790393013,
      "grad_norm": 0.47282353043556213,
      "learning_rate": 0.00017888563049853372,
      "loss": 0.7927,
      "step": 78
    },
    {
      "epoch": 0.11499272197962154,
      "grad_norm": 0.4948686957359314,
      "learning_rate": 0.00017859237536656892,
      "loss": 1.0807,
      "step": 79
    },
    {
      "epoch": 0.11644832605531295,
      "grad_norm": 0.47454243898391724,
      "learning_rate": 0.0001782991202346041,
      "loss": 0.7233,
      "step": 80
    },
    {
      "epoch": 0.11790393013100436,
      "grad_norm": 0.3921898305416107,
      "learning_rate": 0.0001780058651026393,
      "loss": 0.6175,
      "step": 81
    },
    {
      "epoch": 0.11935953420669577,
      "grad_norm": 0.4548739790916443,
      "learning_rate": 0.00017771260997067448,
      "loss": 0.8646,
      "step": 82
    },
    {
      "epoch": 0.12081513828238719,
      "grad_norm": 0.42852383852005005,
      "learning_rate": 0.0001774193548387097,
      "loss": 0.6318,
      "step": 83
    },
    {
      "epoch": 0.1222707423580786,
      "grad_norm": 0.5600367188453674,
      "learning_rate": 0.00017712609970674487,
      "loss": 0.6622,
      "step": 84
    },
    {
      "epoch": 0.12372634643377002,
      "grad_norm": 0.41831374168395996,
      "learning_rate": 0.00017683284457478007,
      "loss": 0.6662,
      "step": 85
    },
    {
      "epoch": 0.12518195050946143,
      "grad_norm": 0.41323322057724,
      "learning_rate": 0.00017653958944281525,
      "loss": 0.8705,
      "step": 86
    },
    {
      "epoch": 0.12663755458515283,
      "grad_norm": 0.45720285177230835,
      "learning_rate": 0.00017624633431085045,
      "loss": 0.8989,
      "step": 87
    },
    {
      "epoch": 0.12809315866084425,
      "grad_norm": 0.5207357406616211,
      "learning_rate": 0.00017595307917888563,
      "loss": 0.9313,
      "step": 88
    },
    {
      "epoch": 0.12954876273653565,
      "grad_norm": 0.4335586428642273,
      "learning_rate": 0.00017565982404692084,
      "loss": 0.7894,
      "step": 89
    },
    {
      "epoch": 0.13100436681222707,
      "grad_norm": 0.4091843366622925,
      "learning_rate": 0.000175366568914956,
      "loss": 0.9236,
      "step": 90
    },
    {
      "epoch": 0.1324599708879185,
      "grad_norm": 0.3710206151008606,
      "learning_rate": 0.00017507331378299122,
      "loss": 0.5849,
      "step": 91
    },
    {
      "epoch": 0.1339155749636099,
      "grad_norm": 0.40621501207351685,
      "learning_rate": 0.0001747800586510264,
      "loss": 0.8725,
      "step": 92
    },
    {
      "epoch": 0.13537117903930132,
      "grad_norm": 0.4465961754322052,
      "learning_rate": 0.0001744868035190616,
      "loss": 0.8438,
      "step": 93
    },
    {
      "epoch": 0.13682678311499272,
      "grad_norm": 0.4613209366798401,
      "learning_rate": 0.00017419354838709678,
      "loss": 1.0112,
      "step": 94
    },
    {
      "epoch": 0.13828238719068414,
      "grad_norm": 0.45989665389060974,
      "learning_rate": 0.00017390029325513198,
      "loss": 0.9326,
      "step": 95
    },
    {
      "epoch": 0.13973799126637554,
      "grad_norm": 0.4962250292301178,
      "learning_rate": 0.00017360703812316716,
      "loss": 0.8243,
      "step": 96
    },
    {
      "epoch": 0.14119359534206696,
      "grad_norm": 0.4414971172809601,
      "learning_rate": 0.00017331378299120236,
      "loss": 0.5982,
      "step": 97
    },
    {
      "epoch": 0.14264919941775836,
      "grad_norm": 0.4271400272846222,
      "learning_rate": 0.00017302052785923754,
      "loss": 0.8883,
      "step": 98
    },
    {
      "epoch": 0.14410480349344978,
      "grad_norm": 0.4753141403198242,
      "learning_rate": 0.00017272727272727275,
      "loss": 0.7996,
      "step": 99
    },
    {
      "epoch": 0.14556040756914118,
      "grad_norm": 0.4174918234348297,
      "learning_rate": 0.00017243401759530792,
      "loss": 0.6085,
      "step": 100
    },
    {
      "epoch": 0.1470160116448326,
      "grad_norm": 0.3893839716911316,
      "learning_rate": 0.00017214076246334313,
      "loss": 0.7351,
      "step": 101
    },
    {
      "epoch": 0.14847161572052403,
      "grad_norm": 0.395735502243042,
      "learning_rate": 0.0001718475073313783,
      "loss": 0.7221,
      "step": 102
    },
    {
      "epoch": 0.14992721979621543,
      "grad_norm": 0.39896532893180847,
      "learning_rate": 0.0001715542521994135,
      "loss": 0.7191,
      "step": 103
    },
    {
      "epoch": 0.15138282387190685,
      "grad_norm": 0.44969457387924194,
      "learning_rate": 0.0001712609970674487,
      "loss": 0.777,
      "step": 104
    },
    {
      "epoch": 0.15283842794759825,
      "grad_norm": 0.49982455372810364,
      "learning_rate": 0.0001709677419354839,
      "loss": 0.7869,
      "step": 105
    },
    {
      "epoch": 0.15429403202328967,
      "grad_norm": 0.3714906871318817,
      "learning_rate": 0.00017067448680351907,
      "loss": 0.6906,
      "step": 106
    },
    {
      "epoch": 0.15574963609898107,
      "grad_norm": 0.4629669487476349,
      "learning_rate": 0.00017038123167155428,
      "loss": 0.9104,
      "step": 107
    },
    {
      "epoch": 0.1572052401746725,
      "grad_norm": 0.37621212005615234,
      "learning_rate": 0.00017008797653958945,
      "loss": 0.8083,
      "step": 108
    },
    {
      "epoch": 0.1586608442503639,
      "grad_norm": 0.4586045444011688,
      "learning_rate": 0.00016979472140762466,
      "loss": 0.8268,
      "step": 109
    },
    {
      "epoch": 0.16011644832605532,
      "grad_norm": 0.4630807340145111,
      "learning_rate": 0.00016950146627565984,
      "loss": 0.7358,
      "step": 110
    },
    {
      "epoch": 0.1615720524017467,
      "grad_norm": 0.39688313007354736,
      "learning_rate": 0.00016920821114369504,
      "loss": 0.664,
      "step": 111
    },
    {
      "epoch": 0.16302765647743814,
      "grad_norm": 0.4408501982688904,
      "learning_rate": 0.00016891495601173022,
      "loss": 1.023,
      "step": 112
    },
    {
      "epoch": 0.16448326055312956,
      "grad_norm": 0.47638335824012756,
      "learning_rate": 0.00016862170087976542,
      "loss": 0.6646,
      "step": 113
    },
    {
      "epoch": 0.16593886462882096,
      "grad_norm": 0.40244197845458984,
      "learning_rate": 0.0001683284457478006,
      "loss": 0.8401,
      "step": 114
    },
    {
      "epoch": 0.16739446870451238,
      "grad_norm": 0.4372684061527252,
      "learning_rate": 0.00016803519061583578,
      "loss": 0.9472,
      "step": 115
    },
    {
      "epoch": 0.16885007278020378,
      "grad_norm": 0.4747982621192932,
      "learning_rate": 0.00016774193548387098,
      "loss": 0.782,
      "step": 116
    },
    {
      "epoch": 0.1703056768558952,
      "grad_norm": 0.5068087577819824,
      "learning_rate": 0.00016744868035190616,
      "loss": 0.9274,
      "step": 117
    },
    {
      "epoch": 0.1717612809315866,
      "grad_norm": 0.4089363217353821,
      "learning_rate": 0.00016715542521994137,
      "loss": 0.868,
      "step": 118
    },
    {
      "epoch": 0.17321688500727803,
      "grad_norm": 0.37761056423187256,
      "learning_rate": 0.00016686217008797654,
      "loss": 0.7918,
      "step": 119
    },
    {
      "epoch": 0.17467248908296942,
      "grad_norm": 0.4098760485649109,
      "learning_rate": 0.00016656891495601175,
      "loss": 0.6553,
      "step": 120
    },
    {
      "epoch": 0.17612809315866085,
      "grad_norm": 0.42955830693244934,
      "learning_rate": 0.00016627565982404693,
      "loss": 0.7655,
      "step": 121
    },
    {
      "epoch": 0.17758369723435224,
      "grad_norm": 0.4503638744354248,
      "learning_rate": 0.00016598240469208213,
      "loss": 0.801,
      "step": 122
    },
    {
      "epoch": 0.17903930131004367,
      "grad_norm": 0.44725823402404785,
      "learning_rate": 0.0001656891495601173,
      "loss": 0.6844,
      "step": 123
    },
    {
      "epoch": 0.1804949053857351,
      "grad_norm": 0.422909140586853,
      "learning_rate": 0.0001653958944281525,
      "loss": 0.6345,
      "step": 124
    },
    {
      "epoch": 0.1819505094614265,
      "grad_norm": 0.4089552164077759,
      "learning_rate": 0.0001651026392961877,
      "loss": 0.8349,
      "step": 125
    },
    {
      "epoch": 0.18340611353711792,
      "grad_norm": 0.42741021513938904,
      "learning_rate": 0.0001648093841642229,
      "loss": 0.7566,
      "step": 126
    },
    {
      "epoch": 0.1848617176128093,
      "grad_norm": 0.4752839505672455,
      "learning_rate": 0.00016451612903225807,
      "loss": 0.747,
      "step": 127
    },
    {
      "epoch": 0.18631732168850074,
      "grad_norm": 0.4625546634197235,
      "learning_rate": 0.00016422287390029328,
      "loss": 0.8026,
      "step": 128
    },
    {
      "epoch": 0.18777292576419213,
      "grad_norm": 0.44090235233306885,
      "learning_rate": 0.00016392961876832845,
      "loss": 0.8863,
      "step": 129
    },
    {
      "epoch": 0.18922852983988356,
      "grad_norm": 0.4268661141395569,
      "learning_rate": 0.00016363636363636366,
      "loss": 0.8087,
      "step": 130
    },
    {
      "epoch": 0.19068413391557495,
      "grad_norm": 0.42549312114715576,
      "learning_rate": 0.00016334310850439884,
      "loss": 0.9065,
      "step": 131
    },
    {
      "epoch": 0.19213973799126638,
      "grad_norm": 0.43122395873069763,
      "learning_rate": 0.00016304985337243404,
      "loss": 0.721,
      "step": 132
    },
    {
      "epoch": 0.19359534206695778,
      "grad_norm": 0.43319809436798096,
      "learning_rate": 0.00016275659824046922,
      "loss": 0.9808,
      "step": 133
    },
    {
      "epoch": 0.1950509461426492,
      "grad_norm": 0.39379703998565674,
      "learning_rate": 0.0001624633431085044,
      "loss": 0.764,
      "step": 134
    },
    {
      "epoch": 0.1965065502183406,
      "grad_norm": 0.4249659478664398,
      "learning_rate": 0.00016217008797653957,
      "loss": 0.861,
      "step": 135
    },
    {
      "epoch": 0.19796215429403202,
      "grad_norm": 0.4942259192466736,
      "learning_rate": 0.00016187683284457478,
      "loss": 0.7534,
      "step": 136
    },
    {
      "epoch": 0.19941775836972345,
      "grad_norm": 0.44764596223831177,
      "learning_rate": 0.00016158357771260996,
      "loss": 0.909,
      "step": 137
    },
    {
      "epoch": 0.20087336244541484,
      "grad_norm": 0.38556259870529175,
      "learning_rate": 0.00016129032258064516,
      "loss": 0.7731,
      "step": 138
    },
    {
      "epoch": 0.20232896652110627,
      "grad_norm": 0.4339657127857208,
      "learning_rate": 0.00016099706744868034,
      "loss": 0.6483,
      "step": 139
    },
    {
      "epoch": 0.20378457059679767,
      "grad_norm": 0.37393108010292053,
      "learning_rate": 0.00016070381231671554,
      "loss": 0.8289,
      "step": 140
    },
    {
      "epoch": 0.2052401746724891,
      "grad_norm": 0.3952410817146301,
      "learning_rate": 0.00016041055718475072,
      "loss": 0.672,
      "step": 141
    },
    {
      "epoch": 0.2066957787481805,
      "grad_norm": 0.5188900828361511,
      "learning_rate": 0.00016011730205278593,
      "loss": 0.6766,
      "step": 142
    },
    {
      "epoch": 0.2081513828238719,
      "grad_norm": 0.43718716502189636,
      "learning_rate": 0.0001598240469208211,
      "loss": 0.9618,
      "step": 143
    },
    {
      "epoch": 0.2096069868995633,
      "grad_norm": 0.4605420231819153,
      "learning_rate": 0.0001595307917888563,
      "loss": 0.751,
      "step": 144
    },
    {
      "epoch": 0.21106259097525473,
      "grad_norm": 0.44414830207824707,
      "learning_rate": 0.00015923753665689149,
      "loss": 0.7855,
      "step": 145
    },
    {
      "epoch": 0.21251819505094613,
      "grad_norm": 0.44049131870269775,
      "learning_rate": 0.0001589442815249267,
      "loss": 0.8832,
      "step": 146
    },
    {
      "epoch": 0.21397379912663755,
      "grad_norm": 0.46913039684295654,
      "learning_rate": 0.00015865102639296187,
      "loss": 0.7641,
      "step": 147
    },
    {
      "epoch": 0.21542940320232898,
      "grad_norm": 0.4389380216598511,
      "learning_rate": 0.00015835777126099707,
      "loss": 0.9718,
      "step": 148
    },
    {
      "epoch": 0.21688500727802038,
      "grad_norm": 0.43579235672950745,
      "learning_rate": 0.00015806451612903225,
      "loss": 0.6582,
      "step": 149
    },
    {
      "epoch": 0.2183406113537118,
      "grad_norm": 0.4359445571899414,
      "learning_rate": 0.00015777126099706746,
      "loss": 0.6021,
      "step": 150
    },
    {
      "epoch": 0.2197962154294032,
      "grad_norm": 0.5175174474716187,
      "learning_rate": 0.00015747800586510263,
      "loss": 0.871,
      "step": 151
    },
    {
      "epoch": 0.22125181950509462,
      "grad_norm": 0.4524837136268616,
      "learning_rate": 0.00015718475073313784,
      "loss": 0.7345,
      "step": 152
    },
    {
      "epoch": 0.22270742358078602,
      "grad_norm": 0.43810054659843445,
      "learning_rate": 0.00015689149560117301,
      "loss": 0.8909,
      "step": 153
    },
    {
      "epoch": 0.22416302765647744,
      "grad_norm": 0.43184036016464233,
      "learning_rate": 0.00015659824046920822,
      "loss": 0.7681,
      "step": 154
    },
    {
      "epoch": 0.22561863173216884,
      "grad_norm": 0.44728273153305054,
      "learning_rate": 0.0001563049853372434,
      "loss": 0.6412,
      "step": 155
    },
    {
      "epoch": 0.22707423580786026,
      "grad_norm": 0.3758560121059418,
      "learning_rate": 0.0001560117302052786,
      "loss": 0.8913,
      "step": 156
    },
    {
      "epoch": 0.22852983988355166,
      "grad_norm": 0.40103447437286377,
      "learning_rate": 0.00015571847507331378,
      "loss": 0.7358,
      "step": 157
    },
    {
      "epoch": 0.22998544395924309,
      "grad_norm": 0.4291788637638092,
      "learning_rate": 0.00015542521994134898,
      "loss": 0.7689,
      "step": 158
    },
    {
      "epoch": 0.2314410480349345,
      "grad_norm": 0.4880374073982239,
      "learning_rate": 0.00015513196480938416,
      "loss": 1.0131,
      "step": 159
    },
    {
      "epoch": 0.2328966521106259,
      "grad_norm": 0.4683997333049774,
      "learning_rate": 0.00015483870967741937,
      "loss": 0.8414,
      "step": 160
    },
    {
      "epoch": 0.23435225618631733,
      "grad_norm": 0.3974555432796478,
      "learning_rate": 0.00015454545454545454,
      "loss": 0.8066,
      "step": 161
    },
    {
      "epoch": 0.23580786026200873,
      "grad_norm": 0.4528520107269287,
      "learning_rate": 0.00015425219941348975,
      "loss": 0.9831,
      "step": 162
    },
    {
      "epoch": 0.23726346433770015,
      "grad_norm": 0.4025037884712219,
      "learning_rate": 0.00015395894428152493,
      "loss": 0.8486,
      "step": 163
    },
    {
      "epoch": 0.23871906841339155,
      "grad_norm": 0.3896133601665497,
      "learning_rate": 0.00015366568914956013,
      "loss": 0.6647,
      "step": 164
    },
    {
      "epoch": 0.24017467248908297,
      "grad_norm": 0.3980013430118561,
      "learning_rate": 0.0001533724340175953,
      "loss": 0.7535,
      "step": 165
    },
    {
      "epoch": 0.24163027656477437,
      "grad_norm": 0.4816455841064453,
      "learning_rate": 0.00015307917888563051,
      "loss": 0.6317,
      "step": 166
    },
    {
      "epoch": 0.2430858806404658,
      "grad_norm": 0.4734821617603302,
      "learning_rate": 0.0001527859237536657,
      "loss": 0.8098,
      "step": 167
    },
    {
      "epoch": 0.2445414847161572,
      "grad_norm": 0.4178393483161926,
      "learning_rate": 0.0001524926686217009,
      "loss": 0.8291,
      "step": 168
    },
    {
      "epoch": 0.24599708879184862,
      "grad_norm": 0.4880813658237457,
      "learning_rate": 0.00015219941348973607,
      "loss": 0.9199,
      "step": 169
    },
    {
      "epoch": 0.24745269286754004,
      "grad_norm": 0.42776724696159363,
      "learning_rate": 0.00015190615835777128,
      "loss": 0.8321,
      "step": 170
    },
    {
      "epoch": 0.24890829694323144,
      "grad_norm": 0.4067710340023041,
      "learning_rate": 0.00015161290322580646,
      "loss": 0.887,
      "step": 171
    },
    {
      "epoch": 0.25036390101892286,
      "grad_norm": 0.4252428412437439,
      "learning_rate": 0.00015131964809384166,
      "loss": 0.7993,
      "step": 172
    },
    {
      "epoch": 0.25181950509461426,
      "grad_norm": 0.4429514408111572,
      "learning_rate": 0.00015102639296187684,
      "loss": 0.7869,
      "step": 173
    },
    {
      "epoch": 0.25327510917030566,
      "grad_norm": 0.39494451880455017,
      "learning_rate": 0.00015073313782991204,
      "loss": 0.8314,
      "step": 174
    },
    {
      "epoch": 0.2547307132459971,
      "grad_norm": 0.4211786985397339,
      "learning_rate": 0.00015043988269794722,
      "loss": 1.0744,
      "step": 175
    },
    {
      "epoch": 0.2561863173216885,
      "grad_norm": 0.44330334663391113,
      "learning_rate": 0.00015014662756598243,
      "loss": 0.945,
      "step": 176
    },
    {
      "epoch": 0.2576419213973799,
      "grad_norm": 0.4130684733390808,
      "learning_rate": 0.0001498533724340176,
      "loss": 0.851,
      "step": 177
    },
    {
      "epoch": 0.2590975254730713,
      "grad_norm": 0.43028414249420166,
      "learning_rate": 0.00014956011730205278,
      "loss": 0.8039,
      "step": 178
    },
    {
      "epoch": 0.26055312954876275,
      "grad_norm": 0.4429122805595398,
      "learning_rate": 0.00014926686217008799,
      "loss": 0.766,
      "step": 179
    },
    {
      "epoch": 0.26200873362445415,
      "grad_norm": 0.3913215398788452,
      "learning_rate": 0.00014897360703812316,
      "loss": 0.7807,
      "step": 180
    },
    {
      "epoch": 0.26346433770014555,
      "grad_norm": 0.437966525554657,
      "learning_rate": 0.00014868035190615837,
      "loss": 0.8004,
      "step": 181
    },
    {
      "epoch": 0.264919941775837,
      "grad_norm": 0.4343196153640747,
      "learning_rate": 0.00014838709677419355,
      "loss": 0.812,
      "step": 182
    },
    {
      "epoch": 0.2663755458515284,
      "grad_norm": 0.41302940249443054,
      "learning_rate": 0.00014809384164222875,
      "loss": 0.812,
      "step": 183
    },
    {
      "epoch": 0.2678311499272198,
      "grad_norm": 0.44081658124923706,
      "learning_rate": 0.00014780058651026393,
      "loss": 0.7492,
      "step": 184
    },
    {
      "epoch": 0.2692867540029112,
      "grad_norm": 0.4287570118904114,
      "learning_rate": 0.00014750733137829913,
      "loss": 0.7808,
      "step": 185
    },
    {
      "epoch": 0.27074235807860264,
      "grad_norm": 0.4003458023071289,
      "learning_rate": 0.0001472140762463343,
      "loss": 0.8996,
      "step": 186
    },
    {
      "epoch": 0.27219796215429404,
      "grad_norm": 0.4149245619773865,
      "learning_rate": 0.00014692082111436951,
      "loss": 0.9305,
      "step": 187
    },
    {
      "epoch": 0.27365356622998543,
      "grad_norm": 0.43740296363830566,
      "learning_rate": 0.0001466275659824047,
      "loss": 0.9329,
      "step": 188
    },
    {
      "epoch": 0.27510917030567683,
      "grad_norm": 0.36761805415153503,
      "learning_rate": 0.0001463343108504399,
      "loss": 0.7157,
      "step": 189
    },
    {
      "epoch": 0.2765647743813683,
      "grad_norm": 0.38137388229370117,
      "learning_rate": 0.00014604105571847507,
      "loss": 0.8158,
      "step": 190
    },
    {
      "epoch": 0.2780203784570597,
      "grad_norm": 0.4645918905735016,
      "learning_rate": 0.00014574780058651028,
      "loss": 1.0011,
      "step": 191
    },
    {
      "epoch": 0.2794759825327511,
      "grad_norm": 0.40438616275787354,
      "learning_rate": 0.00014545454545454546,
      "loss": 0.5236,
      "step": 192
    },
    {
      "epoch": 0.28093158660844253,
      "grad_norm": 0.4237351417541504,
      "learning_rate": 0.00014516129032258066,
      "loss": 0.893,
      "step": 193
    },
    {
      "epoch": 0.2823871906841339,
      "grad_norm": 0.37568819522857666,
      "learning_rate": 0.00014486803519061584,
      "loss": 0.7507,
      "step": 194
    },
    {
      "epoch": 0.2838427947598253,
      "grad_norm": 0.3779682517051697,
      "learning_rate": 0.00014457478005865104,
      "loss": 1.02,
      "step": 195
    },
    {
      "epoch": 0.2852983988355167,
      "grad_norm": 0.42052367329597473,
      "learning_rate": 0.00014428152492668622,
      "loss": 0.8248,
      "step": 196
    },
    {
      "epoch": 0.2867540029112082,
      "grad_norm": 0.4055632948875427,
      "learning_rate": 0.00014398826979472143,
      "loss": 0.8448,
      "step": 197
    },
    {
      "epoch": 0.28820960698689957,
      "grad_norm": 0.41001367568969727,
      "learning_rate": 0.0001436950146627566,
      "loss": 0.7652,
      "step": 198
    },
    {
      "epoch": 0.28966521106259097,
      "grad_norm": 0.45775049924850464,
      "learning_rate": 0.0001434017595307918,
      "loss": 0.8462,
      "step": 199
    },
    {
      "epoch": 0.29112081513828236,
      "grad_norm": 0.44078221917152405,
      "learning_rate": 0.00014310850439882699,
      "loss": 0.6259,
      "step": 200
    },
    {
      "epoch": 0.2925764192139738,
      "grad_norm": 0.38295984268188477,
      "learning_rate": 0.0001428152492668622,
      "loss": 0.6718,
      "step": 201
    },
    {
      "epoch": 0.2940320232896652,
      "grad_norm": 0.39016658067703247,
      "learning_rate": 0.00014252199413489737,
      "loss": 0.7661,
      "step": 202
    },
    {
      "epoch": 0.2954876273653566,
      "grad_norm": 0.4429571330547333,
      "learning_rate": 0.00014222873900293257,
      "loss": 0.9959,
      "step": 203
    },
    {
      "epoch": 0.29694323144104806,
      "grad_norm": 0.49389928579330444,
      "learning_rate": 0.00014193548387096775,
      "loss": 0.8476,
      "step": 204
    },
    {
      "epoch": 0.29839883551673946,
      "grad_norm": 0.4018436074256897,
      "learning_rate": 0.00014164222873900296,
      "loss": 0.9386,
      "step": 205
    },
    {
      "epoch": 0.29985443959243085,
      "grad_norm": 0.39294353127479553,
      "learning_rate": 0.00014134897360703813,
      "loss": 0.7353,
      "step": 206
    },
    {
      "epoch": 0.30131004366812225,
      "grad_norm": 0.43378734588623047,
      "learning_rate": 0.00014105571847507334,
      "loss": 0.7129,
      "step": 207
    },
    {
      "epoch": 0.3027656477438137,
      "grad_norm": 0.36184069514274597,
      "learning_rate": 0.00014076246334310852,
      "loss": 0.7322,
      "step": 208
    },
    {
      "epoch": 0.3042212518195051,
      "grad_norm": 0.40099817514419556,
      "learning_rate": 0.00014046920821114372,
      "loss": 0.7917,
      "step": 209
    },
    {
      "epoch": 0.3056768558951965,
      "grad_norm": 0.3970656394958496,
      "learning_rate": 0.0001401759530791789,
      "loss": 0.7091,
      "step": 210
    },
    {
      "epoch": 0.3071324599708879,
      "grad_norm": 0.4268690049648285,
      "learning_rate": 0.0001398826979472141,
      "loss": 0.9198,
      "step": 211
    },
    {
      "epoch": 0.30858806404657935,
      "grad_norm": 0.37226757407188416,
      "learning_rate": 0.00013958944281524928,
      "loss": 0.8113,
      "step": 212
    },
    {
      "epoch": 0.31004366812227074,
      "grad_norm": 0.647700309753418,
      "learning_rate": 0.00013929618768328448,
      "loss": 0.969,
      "step": 213
    },
    {
      "epoch": 0.31149927219796214,
      "grad_norm": 0.42493534088134766,
      "learning_rate": 0.00013900293255131966,
      "loss": 0.897,
      "step": 214
    },
    {
      "epoch": 0.3129548762736536,
      "grad_norm": 0.3955356776714325,
      "learning_rate": 0.00013870967741935487,
      "loss": 0.8261,
      "step": 215
    },
    {
      "epoch": 0.314410480349345,
      "grad_norm": 0.38680794835090637,
      "learning_rate": 0.00013841642228739004,
      "loss": 0.6503,
      "step": 216
    },
    {
      "epoch": 0.3158660844250364,
      "grad_norm": 0.4285343289375305,
      "learning_rate": 0.00013812316715542525,
      "loss": 0.5602,
      "step": 217
    },
    {
      "epoch": 0.3173216885007278,
      "grad_norm": 0.4221801459789276,
      "learning_rate": 0.00013782991202346043,
      "loss": 0.6224,
      "step": 218
    },
    {
      "epoch": 0.31877729257641924,
      "grad_norm": 0.47130414843559265,
      "learning_rate": 0.00013753665689149563,
      "loss": 0.7611,
      "step": 219
    },
    {
      "epoch": 0.32023289665211063,
      "grad_norm": 0.36859744787216187,
      "learning_rate": 0.00013724340175953078,
      "loss": 0.6053,
      "step": 220
    },
    {
      "epoch": 0.32168850072780203,
      "grad_norm": 0.337546706199646,
      "learning_rate": 0.000136950146627566,
      "loss": 0.6108,
      "step": 221
    },
    {
      "epoch": 0.3231441048034934,
      "grad_norm": 0.38267990946769714,
      "learning_rate": 0.00013665689149560116,
      "loss": 0.8792,
      "step": 222
    },
    {
      "epoch": 0.3245997088791849,
      "grad_norm": 0.4505497217178345,
      "learning_rate": 0.00013636363636363637,
      "loss": 0.8179,
      "step": 223
    },
    {
      "epoch": 0.3260553129548763,
      "grad_norm": 0.42149457335472107,
      "learning_rate": 0.00013607038123167155,
      "loss": 0.8968,
      "step": 224
    },
    {
      "epoch": 0.32751091703056767,
      "grad_norm": 0.37863287329673767,
      "learning_rate": 0.00013577712609970675,
      "loss": 0.8296,
      "step": 225
    },
    {
      "epoch": 0.3289665211062591,
      "grad_norm": 0.4006965756416321,
      "learning_rate": 0.00013548387096774193,
      "loss": 0.6819,
      "step": 226
    },
    {
      "epoch": 0.3304221251819505,
      "grad_norm": 0.42343345284461975,
      "learning_rate": 0.00013519061583577713,
      "loss": 0.8233,
      "step": 227
    },
    {
      "epoch": 0.3318777292576419,
      "grad_norm": 0.3848936855792999,
      "learning_rate": 0.0001348973607038123,
      "loss": 0.6537,
      "step": 228
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.5003016591072083,
      "learning_rate": 0.00013460410557184752,
      "loss": 1.0144,
      "step": 229
    },
    {
      "epoch": 0.33478893740902477,
      "grad_norm": 0.41583725810050964,
      "learning_rate": 0.0001343108504398827,
      "loss": 1.0353,
      "step": 230
    },
    {
      "epoch": 0.33624454148471616,
      "grad_norm": 0.4193093776702881,
      "learning_rate": 0.0001340175953079179,
      "loss": 0.8557,
      "step": 231
    },
    {
      "epoch": 0.33770014556040756,
      "grad_norm": 0.36548593640327454,
      "learning_rate": 0.00013372434017595308,
      "loss": 0.6168,
      "step": 232
    },
    {
      "epoch": 0.33915574963609896,
      "grad_norm": 0.40226423740386963,
      "learning_rate": 0.00013343108504398828,
      "loss": 0.7138,
      "step": 233
    },
    {
      "epoch": 0.3406113537117904,
      "grad_norm": 0.38206160068511963,
      "learning_rate": 0.00013313782991202346,
      "loss": 0.6015,
      "step": 234
    },
    {
      "epoch": 0.3420669577874818,
      "grad_norm": 0.41154223680496216,
      "learning_rate": 0.00013284457478005866,
      "loss": 0.8818,
      "step": 235
    },
    {
      "epoch": 0.3435225618631732,
      "grad_norm": 0.42368200421333313,
      "learning_rate": 0.00013255131964809384,
      "loss": 0.7404,
      "step": 236
    },
    {
      "epoch": 0.34497816593886466,
      "grad_norm": 0.3993453085422516,
      "learning_rate": 0.00013225806451612905,
      "loss": 0.7373,
      "step": 237
    },
    {
      "epoch": 0.34643377001455605,
      "grad_norm": 0.4305897355079651,
      "learning_rate": 0.00013196480938416422,
      "loss": 0.9109,
      "step": 238
    },
    {
      "epoch": 0.34788937409024745,
      "grad_norm": 0.3678435981273651,
      "learning_rate": 0.0001316715542521994,
      "loss": 0.7647,
      "step": 239
    },
    {
      "epoch": 0.34934497816593885,
      "grad_norm": 0.38925084471702576,
      "learning_rate": 0.0001313782991202346,
      "loss": 0.7672,
      "step": 240
    },
    {
      "epoch": 0.3508005822416303,
      "grad_norm": 0.44641968607902527,
      "learning_rate": 0.00013108504398826978,
      "loss": 0.8087,
      "step": 241
    },
    {
      "epoch": 0.3522561863173217,
      "grad_norm": 0.43565723299980164,
      "learning_rate": 0.000130791788856305,
      "loss": 0.8212,
      "step": 242
    },
    {
      "epoch": 0.3537117903930131,
      "grad_norm": 0.436161607503891,
      "learning_rate": 0.00013049853372434016,
      "loss": 0.7419,
      "step": 243
    },
    {
      "epoch": 0.3551673944687045,
      "grad_norm": 0.47400912642478943,
      "learning_rate": 0.00013020527859237537,
      "loss": 0.9181,
      "step": 244
    },
    {
      "epoch": 0.35662299854439594,
      "grad_norm": 0.4325222074985504,
      "learning_rate": 0.00012991202346041055,
      "loss": 0.8424,
      "step": 245
    },
    {
      "epoch": 0.35807860262008734,
      "grad_norm": 0.4145749807357788,
      "learning_rate": 0.00012961876832844575,
      "loss": 0.7307,
      "step": 246
    },
    {
      "epoch": 0.35953420669577874,
      "grad_norm": 0.4017658531665802,
      "learning_rate": 0.00012932551319648093,
      "loss": 0.7351,
      "step": 247
    },
    {
      "epoch": 0.3609898107714702,
      "grad_norm": 0.4188694357872009,
      "learning_rate": 0.00012903225806451613,
      "loss": 0.9676,
      "step": 248
    },
    {
      "epoch": 0.3624454148471616,
      "grad_norm": 0.4340555965900421,
      "learning_rate": 0.0001287390029325513,
      "loss": 0.803,
      "step": 249
    },
    {
      "epoch": 0.363901018922853,
      "grad_norm": 0.42546024918556213,
      "learning_rate": 0.00012844574780058652,
      "loss": 0.7914,
      "step": 250
    },
    {
      "epoch": 0.3653566229985444,
      "grad_norm": 0.39830806851387024,
      "learning_rate": 0.0001281524926686217,
      "loss": 0.8179,
      "step": 251
    },
    {
      "epoch": 0.36681222707423583,
      "grad_norm": 0.40607067942619324,
      "learning_rate": 0.0001278592375366569,
      "loss": 0.8096,
      "step": 252
    },
    {
      "epoch": 0.3682678311499272,
      "grad_norm": 0.3709719479084015,
      "learning_rate": 0.00012756598240469208,
      "loss": 0.6014,
      "step": 253
    },
    {
      "epoch": 0.3697234352256186,
      "grad_norm": 0.4185880124568939,
      "learning_rate": 0.00012727272727272728,
      "loss": 0.5577,
      "step": 254
    },
    {
      "epoch": 0.37117903930131,
      "grad_norm": 0.4391224980354309,
      "learning_rate": 0.00012697947214076246,
      "loss": 0.8096,
      "step": 255
    },
    {
      "epoch": 0.3726346433770015,
      "grad_norm": 0.48843273520469666,
      "learning_rate": 0.00012668621700879766,
      "loss": 0.8085,
      "step": 256
    },
    {
      "epoch": 0.37409024745269287,
      "grad_norm": 0.4689715504646301,
      "learning_rate": 0.00012639296187683284,
      "loss": 0.9994,
      "step": 257
    },
    {
      "epoch": 0.37554585152838427,
      "grad_norm": 0.4412029981613159,
      "learning_rate": 0.00012609970674486805,
      "loss": 0.6314,
      "step": 258
    },
    {
      "epoch": 0.37700145560407566,
      "grad_norm": 0.4322951138019562,
      "learning_rate": 0.00012580645161290322,
      "loss": 0.7756,
      "step": 259
    },
    {
      "epoch": 0.3784570596797671,
      "grad_norm": 0.38179484009742737,
      "learning_rate": 0.00012551319648093843,
      "loss": 0.7408,
      "step": 260
    },
    {
      "epoch": 0.3799126637554585,
      "grad_norm": 0.39342549443244934,
      "learning_rate": 0.0001252199413489736,
      "loss": 0.7123,
      "step": 261
    },
    {
      "epoch": 0.3813682678311499,
      "grad_norm": 0.37399575114250183,
      "learning_rate": 0.0001249266862170088,
      "loss": 0.7527,
      "step": 262
    },
    {
      "epoch": 0.38282387190684136,
      "grad_norm": 0.37510353326797485,
      "learning_rate": 0.000124633431085044,
      "loss": 0.6711,
      "step": 263
    },
    {
      "epoch": 0.38427947598253276,
      "grad_norm": 0.37771669030189514,
      "learning_rate": 0.0001243401759530792,
      "loss": 0.7025,
      "step": 264
    },
    {
      "epoch": 0.38573508005822416,
      "grad_norm": 0.3893979489803314,
      "learning_rate": 0.00012404692082111437,
      "loss": 0.9826,
      "step": 265
    },
    {
      "epoch": 0.38719068413391555,
      "grad_norm": 0.4039032459259033,
      "learning_rate": 0.00012375366568914958,
      "loss": 1.1332,
      "step": 266
    },
    {
      "epoch": 0.388646288209607,
      "grad_norm": 0.36216047406196594,
      "learning_rate": 0.00012346041055718475,
      "loss": 0.6243,
      "step": 267
    },
    {
      "epoch": 0.3901018922852984,
      "grad_norm": 0.39282092452049255,
      "learning_rate": 0.00012316715542521996,
      "loss": 0.8508,
      "step": 268
    },
    {
      "epoch": 0.3915574963609898,
      "grad_norm": 0.4042051434516907,
      "learning_rate": 0.00012287390029325514,
      "loss": 0.5775,
      "step": 269
    },
    {
      "epoch": 0.3930131004366812,
      "grad_norm": 0.4772563576698303,
      "learning_rate": 0.00012258064516129034,
      "loss": 1.0618,
      "step": 270
    },
    {
      "epoch": 0.39446870451237265,
      "grad_norm": 0.37979671359062195,
      "learning_rate": 0.00012228739002932552,
      "loss": 0.7398,
      "step": 271
    },
    {
      "epoch": 0.39592430858806404,
      "grad_norm": 0.3874838352203369,
      "learning_rate": 0.00012199413489736071,
      "loss": 0.7308,
      "step": 272
    },
    {
      "epoch": 0.39737991266375544,
      "grad_norm": 0.3921584188938141,
      "learning_rate": 0.0001217008797653959,
      "loss": 0.6774,
      "step": 273
    },
    {
      "epoch": 0.3988355167394469,
      "grad_norm": 0.3865256607532501,
      "learning_rate": 0.00012140762463343109,
      "loss": 0.7022,
      "step": 274
    },
    {
      "epoch": 0.4002911208151383,
      "grad_norm": 0.37546196579933167,
      "learning_rate": 0.00012111436950146628,
      "loss": 0.8473,
      "step": 275
    },
    {
      "epoch": 0.4017467248908297,
      "grad_norm": 0.4141806960105896,
      "learning_rate": 0.00012082111436950147,
      "loss": 0.7244,
      "step": 276
    },
    {
      "epoch": 0.4032023289665211,
      "grad_norm": 0.4658057391643524,
      "learning_rate": 0.00012052785923753666,
      "loss": 0.7343,
      "step": 277
    },
    {
      "epoch": 0.40465793304221254,
      "grad_norm": 0.40297892689704895,
      "learning_rate": 0.00012023460410557186,
      "loss": 0.8396,
      "step": 278
    },
    {
      "epoch": 0.40611353711790393,
      "grad_norm": 0.46634405851364136,
      "learning_rate": 0.00011994134897360705,
      "loss": 1.0129,
      "step": 279
    },
    {
      "epoch": 0.40756914119359533,
      "grad_norm": 0.424870103597641,
      "learning_rate": 0.00011964809384164224,
      "loss": 0.862,
      "step": 280
    },
    {
      "epoch": 0.4090247452692867,
      "grad_norm": 0.44829508662223816,
      "learning_rate": 0.00011935483870967743,
      "loss": 1.0943,
      "step": 281
    },
    {
      "epoch": 0.4104803493449782,
      "grad_norm": 0.38355204463005066,
      "learning_rate": 0.00011906158357771262,
      "loss": 0.7857,
      "step": 282
    },
    {
      "epoch": 0.4119359534206696,
      "grad_norm": 0.4218525290489197,
      "learning_rate": 0.00011876832844574781,
      "loss": 0.7355,
      "step": 283
    },
    {
      "epoch": 0.413391557496361,
      "grad_norm": 0.3925118148326874,
      "learning_rate": 0.000118475073313783,
      "loss": 0.8731,
      "step": 284
    },
    {
      "epoch": 0.4148471615720524,
      "grad_norm": 0.3985912799835205,
      "learning_rate": 0.0001181818181818182,
      "loss": 0.9341,
      "step": 285
    },
    {
      "epoch": 0.4163027656477438,
      "grad_norm": 0.4485037326812744,
      "learning_rate": 0.00011788856304985338,
      "loss": 0.8884,
      "step": 286
    },
    {
      "epoch": 0.4177583697234352,
      "grad_norm": 0.3778381645679474,
      "learning_rate": 0.00011759530791788858,
      "loss": 0.5505,
      "step": 287
    },
    {
      "epoch": 0.4192139737991266,
      "grad_norm": 0.3946888744831085,
      "learning_rate": 0.00011730205278592377,
      "loss": 0.929,
      "step": 288
    },
    {
      "epoch": 0.42066957787481807,
      "grad_norm": 0.3854977786540985,
      "learning_rate": 0.00011700879765395896,
      "loss": 0.7851,
      "step": 289
    },
    {
      "epoch": 0.42212518195050946,
      "grad_norm": 0.37854820489883423,
      "learning_rate": 0.00011671554252199415,
      "loss": 0.7698,
      "step": 290
    },
    {
      "epoch": 0.42358078602620086,
      "grad_norm": 0.4082932770252228,
      "learning_rate": 0.00011642228739002934,
      "loss": 0.6568,
      "step": 291
    },
    {
      "epoch": 0.42503639010189226,
      "grad_norm": 0.42098093032836914,
      "learning_rate": 0.00011612903225806453,
      "loss": 0.7233,
      "step": 292
    },
    {
      "epoch": 0.4264919941775837,
      "grad_norm": 0.3610784411430359,
      "learning_rate": 0.00011583577712609972,
      "loss": 1.0027,
      "step": 293
    },
    {
      "epoch": 0.4279475982532751,
      "grad_norm": 0.4008139669895172,
      "learning_rate": 0.00011554252199413491,
      "loss": 0.6112,
      "step": 294
    },
    {
      "epoch": 0.4294032023289665,
      "grad_norm": 0.397676557302475,
      "learning_rate": 0.0001152492668621701,
      "loss": 0.7523,
      "step": 295
    },
    {
      "epoch": 0.43085880640465796,
      "grad_norm": 0.42880338430404663,
      "learning_rate": 0.0001149560117302053,
      "loss": 0.9529,
      "step": 296
    },
    {
      "epoch": 0.43231441048034935,
      "grad_norm": 0.43515437841415405,
      "learning_rate": 0.00011466275659824049,
      "loss": 0.8156,
      "step": 297
    },
    {
      "epoch": 0.43377001455604075,
      "grad_norm": 0.41969555616378784,
      "learning_rate": 0.00011436950146627568,
      "loss": 0.9581,
      "step": 298
    },
    {
      "epoch": 0.43522561863173215,
      "grad_norm": 0.44921761751174927,
      "learning_rate": 0.00011407624633431087,
      "loss": 0.997,
      "step": 299
    },
    {
      "epoch": 0.4366812227074236,
      "grad_norm": 0.38474878668785095,
      "learning_rate": 0.00011378299120234606,
      "loss": 0.6768,
      "step": 300
    },
    {
      "epoch": 0.438136826783115,
      "grad_norm": 0.3799462914466858,
      "learning_rate": 0.00011348973607038125,
      "loss": 0.8426,
      "step": 301
    },
    {
      "epoch": 0.4395924308588064,
      "grad_norm": 0.47940272092819214,
      "learning_rate": 0.00011319648093841644,
      "loss": 0.7784,
      "step": 302
    },
    {
      "epoch": 0.4410480349344978,
      "grad_norm": 0.42794370651245117,
      "learning_rate": 0.00011290322580645163,
      "loss": 0.6684,
      "step": 303
    },
    {
      "epoch": 0.44250363901018924,
      "grad_norm": 0.4973528981208801,
      "learning_rate": 0.00011260997067448683,
      "loss": 0.975,
      "step": 304
    },
    {
      "epoch": 0.44395924308588064,
      "grad_norm": 0.4486595690250397,
      "learning_rate": 0.00011231671554252199,
      "loss": 0.9281,
      "step": 305
    },
    {
      "epoch": 0.44541484716157204,
      "grad_norm": 0.4703066945075989,
      "learning_rate": 0.00011202346041055718,
      "loss": 0.8087,
      "step": 306
    },
    {
      "epoch": 0.4468704512372635,
      "grad_norm": 0.413881778717041,
      "learning_rate": 0.00011173020527859237,
      "loss": 0.6872,
      "step": 307
    },
    {
      "epoch": 0.4483260553129549,
      "grad_norm": 0.3868919610977173,
      "learning_rate": 0.00011143695014662756,
      "loss": 0.799,
      "step": 308
    },
    {
      "epoch": 0.4497816593886463,
      "grad_norm": 0.39034658670425415,
      "learning_rate": 0.00011114369501466275,
      "loss": 0.8858,
      "step": 309
    },
    {
      "epoch": 0.4512372634643377,
      "grad_norm": 0.3317318856716156,
      "learning_rate": 0.00011085043988269795,
      "loss": 0.6194,
      "step": 310
    },
    {
      "epoch": 0.45269286754002913,
      "grad_norm": 0.3944086730480194,
      "learning_rate": 0.00011055718475073314,
      "loss": 0.7831,
      "step": 311
    },
    {
      "epoch": 0.45414847161572053,
      "grad_norm": 0.41719934344291687,
      "learning_rate": 0.00011026392961876833,
      "loss": 0.7303,
      "step": 312
    },
    {
      "epoch": 0.4556040756914119,
      "grad_norm": 0.37548407912254333,
      "learning_rate": 0.00010997067448680352,
      "loss": 0.8779,
      "step": 313
    },
    {
      "epoch": 0.4570596797671033,
      "grad_norm": 0.37348616123199463,
      "learning_rate": 0.00010967741935483871,
      "loss": 0.8278,
      "step": 314
    },
    {
      "epoch": 0.4585152838427948,
      "grad_norm": 0.4360499978065491,
      "learning_rate": 0.0001093841642228739,
      "loss": 0.794,
      "step": 315
    },
    {
      "epoch": 0.45997088791848617,
      "grad_norm": 0.4139132797718048,
      "learning_rate": 0.00010909090909090909,
      "loss": 0.845,
      "step": 316
    },
    {
      "epoch": 0.46142649199417757,
      "grad_norm": 0.3925943970680237,
      "learning_rate": 0.00010879765395894428,
      "loss": 0.7924,
      "step": 317
    },
    {
      "epoch": 0.462882096069869,
      "grad_norm": 0.39502057433128357,
      "learning_rate": 0.00010850439882697947,
      "loss": 0.8534,
      "step": 318
    },
    {
      "epoch": 0.4643377001455604,
      "grad_norm": 0.45057016611099243,
      "learning_rate": 0.00010821114369501467,
      "loss": 0.8046,
      "step": 319
    },
    {
      "epoch": 0.4657933042212518,
      "grad_norm": 0.3781816363334656,
      "learning_rate": 0.00010791788856304986,
      "loss": 0.5689,
      "step": 320
    },
    {
      "epoch": 0.4672489082969432,
      "grad_norm": 0.4176581799983978,
      "learning_rate": 0.00010762463343108505,
      "loss": 0.7567,
      "step": 321
    },
    {
      "epoch": 0.46870451237263466,
      "grad_norm": 0.3789612054824829,
      "learning_rate": 0.00010733137829912024,
      "loss": 0.7263,
      "step": 322
    },
    {
      "epoch": 0.47016011644832606,
      "grad_norm": 0.4175270199775696,
      "learning_rate": 0.00010703812316715543,
      "loss": 0.9061,
      "step": 323
    },
    {
      "epoch": 0.47161572052401746,
      "grad_norm": 0.501302182674408,
      "learning_rate": 0.00010674486803519062,
      "loss": 1.0309,
      "step": 324
    },
    {
      "epoch": 0.47307132459970885,
      "grad_norm": 0.37476909160614014,
      "learning_rate": 0.0001064516129032258,
      "loss": 0.7464,
      "step": 325
    },
    {
      "epoch": 0.4745269286754003,
      "grad_norm": 0.4259251654148102,
      "learning_rate": 0.00010615835777126099,
      "loss": 0.7518,
      "step": 326
    },
    {
      "epoch": 0.4759825327510917,
      "grad_norm": 0.39108017086982727,
      "learning_rate": 0.00010586510263929618,
      "loss": 0.7924,
      "step": 327
    },
    {
      "epoch": 0.4774381368267831,
      "grad_norm": 0.3976758122444153,
      "learning_rate": 0.00010557184750733137,
      "loss": 0.8528,
      "step": 328
    },
    {
      "epoch": 0.47889374090247455,
      "grad_norm": 0.3751293122768402,
      "learning_rate": 0.00010527859237536656,
      "loss": 0.6643,
      "step": 329
    },
    {
      "epoch": 0.48034934497816595,
      "grad_norm": 0.4097519516944885,
      "learning_rate": 0.00010498533724340176,
      "loss": 0.9853,
      "step": 330
    },
    {
      "epoch": 0.48180494905385735,
      "grad_norm": 0.4724092185497284,
      "learning_rate": 0.00010469208211143695,
      "loss": 0.9125,
      "step": 331
    },
    {
      "epoch": 0.48326055312954874,
      "grad_norm": 0.39296412467956543,
      "learning_rate": 0.00010439882697947214,
      "loss": 0.8009,
      "step": 332
    },
    {
      "epoch": 0.4847161572052402,
      "grad_norm": 0.3977135717868805,
      "learning_rate": 0.00010410557184750733,
      "loss": 0.7059,
      "step": 333
    },
    {
      "epoch": 0.4861717612809316,
      "grad_norm": 0.46343791484832764,
      "learning_rate": 0.00010381231671554252,
      "loss": 0.7142,
      "step": 334
    },
    {
      "epoch": 0.487627365356623,
      "grad_norm": 0.43983030319213867,
      "learning_rate": 0.00010351906158357771,
      "loss": 0.7625,
      "step": 335
    },
    {
      "epoch": 0.4890829694323144,
      "grad_norm": 0.3700045943260193,
      "learning_rate": 0.0001032258064516129,
      "loss": 0.7816,
      "step": 336
    },
    {
      "epoch": 0.49053857350800584,
      "grad_norm": 0.4455670416355133,
      "learning_rate": 0.00010293255131964809,
      "loss": 0.8098,
      "step": 337
    },
    {
      "epoch": 0.49199417758369723,
      "grad_norm": 0.43883511424064636,
      "learning_rate": 0.00010263929618768328,
      "loss": 0.6981,
      "step": 338
    },
    {
      "epoch": 0.49344978165938863,
      "grad_norm": 0.39338523149490356,
      "learning_rate": 0.00010234604105571848,
      "loss": 0.8458,
      "step": 339
    },
    {
      "epoch": 0.4949053857350801,
      "grad_norm": 0.4283663332462311,
      "learning_rate": 0.00010205278592375367,
      "loss": 0.5912,
      "step": 340
    },
    {
      "epoch": 0.4963609898107715,
      "grad_norm": 0.39138662815093994,
      "learning_rate": 0.00010175953079178886,
      "loss": 0.8027,
      "step": 341
    },
    {
      "epoch": 0.4978165938864629,
      "grad_norm": 0.41849690675735474,
      "learning_rate": 0.00010146627565982405,
      "loss": 0.7655,
      "step": 342
    },
    {
      "epoch": 0.4992721979621543,
      "grad_norm": 0.3836139142513275,
      "learning_rate": 0.00010117302052785924,
      "loss": 0.776,
      "step": 343
    },
    {
      "epoch": 0.5007278020378457,
      "grad_norm": 0.4218156337738037,
      "learning_rate": 0.00010087976539589443,
      "loss": 0.8946,
      "step": 344
    },
    {
      "epoch": 0.5021834061135371,
      "grad_norm": 0.38104674220085144,
      "learning_rate": 0.00010058651026392962,
      "loss": 0.8357,
      "step": 345
    },
    {
      "epoch": 0.5036390101892285,
      "grad_norm": 0.4143122434616089,
      "learning_rate": 0.00010029325513196481,
      "loss": 0.9398,
      "step": 346
    },
    {
      "epoch": 0.50509461426492,
      "grad_norm": 0.4110569357872009,
      "learning_rate": 0.0001,
      "loss": 0.8446,
      "step": 347
    },
    {
      "epoch": 0.5065502183406113,
      "grad_norm": 0.4693763256072998,
      "learning_rate": 9.97067448680352e-05,
      "loss": 0.911,
      "step": 348
    },
    {
      "epoch": 0.5080058224163028,
      "grad_norm": 0.3450484275817871,
      "learning_rate": 9.941348973607039e-05,
      "loss": 0.6214,
      "step": 349
    },
    {
      "epoch": 0.5094614264919942,
      "grad_norm": 0.43994271755218506,
      "learning_rate": 9.912023460410558e-05,
      "loss": 0.7588,
      "step": 350
    },
    {
      "epoch": 0.5109170305676856,
      "grad_norm": 0.40877294540405273,
      "learning_rate": 9.882697947214077e-05,
      "loss": 0.8974,
      "step": 351
    },
    {
      "epoch": 0.512372634643377,
      "grad_norm": 0.4086593687534332,
      "learning_rate": 9.853372434017596e-05,
      "loss": 0.758,
      "step": 352
    },
    {
      "epoch": 0.5138282387190685,
      "grad_norm": 0.4272790849208832,
      "learning_rate": 9.824046920821115e-05,
      "loss": 0.8793,
      "step": 353
    },
    {
      "epoch": 0.5152838427947598,
      "grad_norm": 0.40227895975112915,
      "learning_rate": 9.794721407624634e-05,
      "loss": 0.9092,
      "step": 354
    },
    {
      "epoch": 0.5167394468704513,
      "grad_norm": 0.4219314754009247,
      "learning_rate": 9.765395894428153e-05,
      "loss": 0.7841,
      "step": 355
    },
    {
      "epoch": 0.5181950509461426,
      "grad_norm": 0.4163002073764801,
      "learning_rate": 9.736070381231673e-05,
      "loss": 0.6177,
      "step": 356
    },
    {
      "epoch": 0.519650655021834,
      "grad_norm": 0.39054110646247864,
      "learning_rate": 9.706744868035192e-05,
      "loss": 0.7865,
      "step": 357
    },
    {
      "epoch": 0.5211062590975255,
      "grad_norm": 0.38139477372169495,
      "learning_rate": 9.677419354838711e-05,
      "loss": 0.7989,
      "step": 358
    },
    {
      "epoch": 0.5225618631732168,
      "grad_norm": 0.4325132668018341,
      "learning_rate": 9.64809384164223e-05,
      "loss": 0.7567,
      "step": 359
    },
    {
      "epoch": 0.5240174672489083,
      "grad_norm": 0.6632893681526184,
      "learning_rate": 9.618768328445749e-05,
      "loss": 0.8514,
      "step": 360
    },
    {
      "epoch": 0.5254730713245997,
      "grad_norm": 0.3502216041088104,
      "learning_rate": 9.589442815249268e-05,
      "loss": 0.829,
      "step": 361
    },
    {
      "epoch": 0.5269286754002911,
      "grad_norm": 0.35933345556259155,
      "learning_rate": 9.560117302052787e-05,
      "loss": 0.7874,
      "step": 362
    },
    {
      "epoch": 0.5283842794759825,
      "grad_norm": 0.36384686827659607,
      "learning_rate": 9.530791788856306e-05,
      "loss": 0.5382,
      "step": 363
    },
    {
      "epoch": 0.529839883551674,
      "grad_norm": 0.4315112829208374,
      "learning_rate": 9.501466275659825e-05,
      "loss": 0.6801,
      "step": 364
    },
    {
      "epoch": 0.5312954876273653,
      "grad_norm": 0.4184367060661316,
      "learning_rate": 9.472140762463345e-05,
      "loss": 0.9261,
      "step": 365
    },
    {
      "epoch": 0.5327510917030568,
      "grad_norm": 0.39874032139778137,
      "learning_rate": 9.442815249266864e-05,
      "loss": 0.8009,
      "step": 366
    },
    {
      "epoch": 0.5342066957787481,
      "grad_norm": 0.4132585823535919,
      "learning_rate": 9.413489736070383e-05,
      "loss": 0.7273,
      "step": 367
    },
    {
      "epoch": 0.5356622998544396,
      "grad_norm": 0.4578113257884979,
      "learning_rate": 9.384164222873902e-05,
      "loss": 1.1526,
      "step": 368
    },
    {
      "epoch": 0.537117903930131,
      "grad_norm": 0.41122883558273315,
      "learning_rate": 9.35483870967742e-05,
      "loss": 0.7026,
      "step": 369
    },
    {
      "epoch": 0.5385735080058224,
      "grad_norm": 0.38746801018714905,
      "learning_rate": 9.325513196480939e-05,
      "loss": 0.8502,
      "step": 370
    },
    {
      "epoch": 0.5400291120815138,
      "grad_norm": 0.49571555852890015,
      "learning_rate": 9.296187683284458e-05,
      "loss": 0.7471,
      "step": 371
    },
    {
      "epoch": 0.5414847161572053,
      "grad_norm": 0.40761005878448486,
      "learning_rate": 9.266862170087977e-05,
      "loss": 0.7174,
      "step": 372
    },
    {
      "epoch": 0.5429403202328966,
      "grad_norm": 0.44395506381988525,
      "learning_rate": 9.237536656891496e-05,
      "loss": 0.9971,
      "step": 373
    },
    {
      "epoch": 0.5443959243085881,
      "grad_norm": 0.4460468292236328,
      "learning_rate": 9.208211143695015e-05,
      "loss": 0.9743,
      "step": 374
    },
    {
      "epoch": 0.5458515283842795,
      "grad_norm": 0.36665448546409607,
      "learning_rate": 9.178885630498534e-05,
      "loss": 0.7472,
      "step": 375
    },
    {
      "epoch": 0.5473071324599709,
      "grad_norm": 0.40692993998527527,
      "learning_rate": 9.149560117302053e-05,
      "loss": 0.7303,
      "step": 376
    },
    {
      "epoch": 0.5487627365356623,
      "grad_norm": 0.41075170040130615,
      "learning_rate": 9.120234604105573e-05,
      "loss": 0.5645,
      "step": 377
    },
    {
      "epoch": 0.5502183406113537,
      "grad_norm": 0.4055960178375244,
      "learning_rate": 9.090909090909092e-05,
      "loss": 0.803,
      "step": 378
    },
    {
      "epoch": 0.5516739446870451,
      "grad_norm": 0.42486807703971863,
      "learning_rate": 9.06158357771261e-05,
      "loss": 0.7623,
      "step": 379
    },
    {
      "epoch": 0.5531295487627366,
      "grad_norm": 0.3824828267097473,
      "learning_rate": 9.032258064516129e-05,
      "loss": 0.7723,
      "step": 380
    },
    {
      "epoch": 0.5545851528384279,
      "grad_norm": 0.3995726704597473,
      "learning_rate": 9.002932551319648e-05,
      "loss": 0.8208,
      "step": 381
    },
    {
      "epoch": 0.5560407569141194,
      "grad_norm": 0.40022045373916626,
      "learning_rate": 8.973607038123167e-05,
      "loss": 0.7855,
      "step": 382
    },
    {
      "epoch": 0.5574963609898108,
      "grad_norm": 0.3811730146408081,
      "learning_rate": 8.944281524926686e-05,
      "loss": 0.5799,
      "step": 383
    },
    {
      "epoch": 0.5589519650655022,
      "grad_norm": 0.4386371970176697,
      "learning_rate": 8.914956011730205e-05,
      "loss": 1.0355,
      "step": 384
    },
    {
      "epoch": 0.5604075691411936,
      "grad_norm": 0.40011438727378845,
      "learning_rate": 8.885630498533724e-05,
      "loss": 0.8693,
      "step": 385
    },
    {
      "epoch": 0.5618631732168851,
      "grad_norm": 0.4194369912147522,
      "learning_rate": 8.856304985337243e-05,
      "loss": 0.6463,
      "step": 386
    },
    {
      "epoch": 0.5633187772925764,
      "grad_norm": 0.43462562561035156,
      "learning_rate": 8.826979472140762e-05,
      "loss": 0.8594,
      "step": 387
    },
    {
      "epoch": 0.5647743813682679,
      "grad_norm": 0.44031593203544617,
      "learning_rate": 8.797653958944282e-05,
      "loss": 0.9846,
      "step": 388
    },
    {
      "epoch": 0.5662299854439592,
      "grad_norm": 0.42791396379470825,
      "learning_rate": 8.7683284457478e-05,
      "loss": 0.9326,
      "step": 389
    },
    {
      "epoch": 0.5676855895196506,
      "grad_norm": 0.36915916204452515,
      "learning_rate": 8.73900293255132e-05,
      "loss": 0.8021,
      "step": 390
    },
    {
      "epoch": 0.5691411935953421,
      "grad_norm": 0.41637444496154785,
      "learning_rate": 8.709677419354839e-05,
      "loss": 0.8929,
      "step": 391
    },
    {
      "epoch": 0.5705967976710334,
      "grad_norm": 0.44747695326805115,
      "learning_rate": 8.680351906158358e-05,
      "loss": 0.7613,
      "step": 392
    },
    {
      "epoch": 0.5720524017467249,
      "grad_norm": 0.4189535975456238,
      "learning_rate": 8.651026392961877e-05,
      "loss": 0.8037,
      "step": 393
    },
    {
      "epoch": 0.5735080058224163,
      "grad_norm": 0.4108138084411621,
      "learning_rate": 8.621700879765396e-05,
      "loss": 1.0073,
      "step": 394
    },
    {
      "epoch": 0.5749636098981077,
      "grad_norm": 0.4459536075592041,
      "learning_rate": 8.592375366568915e-05,
      "loss": 0.8769,
      "step": 395
    },
    {
      "epoch": 0.5764192139737991,
      "grad_norm": 0.3316381275653839,
      "learning_rate": 8.563049853372434e-05,
      "loss": 0.7715,
      "step": 396
    },
    {
      "epoch": 0.5778748180494906,
      "grad_norm": 0.38263294100761414,
      "learning_rate": 8.533724340175954e-05,
      "loss": 0.5865,
      "step": 397
    },
    {
      "epoch": 0.5793304221251819,
      "grad_norm": 0.36469927430152893,
      "learning_rate": 8.504398826979473e-05,
      "loss": 0.7805,
      "step": 398
    },
    {
      "epoch": 0.5807860262008734,
      "grad_norm": 0.3969612717628479,
      "learning_rate": 8.475073313782992e-05,
      "loss": 0.7656,
      "step": 399
    },
    {
      "epoch": 0.5822416302765647,
      "grad_norm": 0.4214104413986206,
      "learning_rate": 8.445747800586511e-05,
      "loss": 0.5687,
      "step": 400
    },
    {
      "epoch": 0.5836972343522562,
      "grad_norm": 0.5265637040138245,
      "learning_rate": 8.41642228739003e-05,
      "loss": 0.7819,
      "step": 401
    },
    {
      "epoch": 0.5851528384279476,
      "grad_norm": 0.41204723715782166,
      "learning_rate": 8.387096774193549e-05,
      "loss": 0.6311,
      "step": 402
    },
    {
      "epoch": 0.586608442503639,
      "grad_norm": 0.38698071241378784,
      "learning_rate": 8.357771260997068e-05,
      "loss": 0.7327,
      "step": 403
    },
    {
      "epoch": 0.5880640465793304,
      "grad_norm": 0.42747408151626587,
      "learning_rate": 8.328445747800587e-05,
      "loss": 0.799,
      "step": 404
    },
    {
      "epoch": 0.5895196506550219,
      "grad_norm": 0.42649203538894653,
      "learning_rate": 8.299120234604106e-05,
      "loss": 0.8559,
      "step": 405
    },
    {
      "epoch": 0.5909752547307132,
      "grad_norm": 0.3997889757156372,
      "learning_rate": 8.269794721407626e-05,
      "loss": 0.7684,
      "step": 406
    },
    {
      "epoch": 0.5924308588064047,
      "grad_norm": 0.3981584906578064,
      "learning_rate": 8.240469208211145e-05,
      "loss": 1.0901,
      "step": 407
    },
    {
      "epoch": 0.5938864628820961,
      "grad_norm": 0.3775689899921417,
      "learning_rate": 8.211143695014664e-05,
      "loss": 0.7341,
      "step": 408
    },
    {
      "epoch": 0.5953420669577875,
      "grad_norm": 0.4539912939071655,
      "learning_rate": 8.181818181818183e-05,
      "loss": 0.701,
      "step": 409
    },
    {
      "epoch": 0.5967976710334789,
      "grad_norm": 0.43960654735565186,
      "learning_rate": 8.152492668621702e-05,
      "loss": 0.7826,
      "step": 410
    },
    {
      "epoch": 0.5982532751091703,
      "grad_norm": 0.4349391460418701,
      "learning_rate": 8.12316715542522e-05,
      "loss": 0.7434,
      "step": 411
    },
    {
      "epoch": 0.5997088791848617,
      "grad_norm": 0.37807536125183105,
      "learning_rate": 8.093841642228739e-05,
      "loss": 0.7091,
      "step": 412
    },
    {
      "epoch": 0.6011644832605532,
      "grad_norm": 0.4363325238227844,
      "learning_rate": 8.064516129032258e-05,
      "loss": 0.821,
      "step": 413
    },
    {
      "epoch": 0.6026200873362445,
      "grad_norm": 0.3981860876083374,
      "learning_rate": 8.035190615835777e-05,
      "loss": 0.72,
      "step": 414
    },
    {
      "epoch": 0.604075691411936,
      "grad_norm": 0.3995717763900757,
      "learning_rate": 8.005865102639296e-05,
      "loss": 0.7446,
      "step": 415
    },
    {
      "epoch": 0.6055312954876274,
      "grad_norm": 0.44248852133750916,
      "learning_rate": 7.976539589442815e-05,
      "loss": 0.8329,
      "step": 416
    },
    {
      "epoch": 0.6069868995633187,
      "grad_norm": 0.37239164113998413,
      "learning_rate": 7.947214076246335e-05,
      "loss": 0.8928,
      "step": 417
    },
    {
      "epoch": 0.6084425036390102,
      "grad_norm": 0.4687086045742035,
      "learning_rate": 7.917888563049854e-05,
      "loss": 0.8467,
      "step": 418
    },
    {
      "epoch": 0.6098981077147017,
      "grad_norm": 0.4026376008987427,
      "learning_rate": 7.888563049853373e-05,
      "loss": 0.738,
      "step": 419
    },
    {
      "epoch": 0.611353711790393,
      "grad_norm": 0.42847108840942383,
      "learning_rate": 7.859237536656892e-05,
      "loss": 0.9711,
      "step": 420
    },
    {
      "epoch": 0.6128093158660844,
      "grad_norm": 0.360431045293808,
      "learning_rate": 7.829912023460411e-05,
      "loss": 0.7124,
      "step": 421
    },
    {
      "epoch": 0.6142649199417758,
      "grad_norm": 0.4211618900299072,
      "learning_rate": 7.80058651026393e-05,
      "loss": 0.6291,
      "step": 422
    },
    {
      "epoch": 0.6157205240174672,
      "grad_norm": 0.4011070132255554,
      "learning_rate": 7.771260997067449e-05,
      "loss": 0.7335,
      "step": 423
    },
    {
      "epoch": 0.6171761280931587,
      "grad_norm": 0.3656633794307709,
      "learning_rate": 7.741935483870968e-05,
      "loss": 0.8247,
      "step": 424
    },
    {
      "epoch": 0.61863173216885,
      "grad_norm": 0.43638092279434204,
      "learning_rate": 7.712609970674487e-05,
      "loss": 0.8656,
      "step": 425
    },
    {
      "epoch": 0.6200873362445415,
      "grad_norm": 0.4566809833049774,
      "learning_rate": 7.683284457478007e-05,
      "loss": 1.0561,
      "step": 426
    },
    {
      "epoch": 0.6215429403202329,
      "grad_norm": 0.3836497962474823,
      "learning_rate": 7.653958944281526e-05,
      "loss": 0.7721,
      "step": 427
    },
    {
      "epoch": 0.6229985443959243,
      "grad_norm": 0.3756355047225952,
      "learning_rate": 7.624633431085045e-05,
      "loss": 0.7506,
      "step": 428
    },
    {
      "epoch": 0.6244541484716157,
      "grad_norm": 0.4741055369377136,
      "learning_rate": 7.595307917888564e-05,
      "loss": 0.9815,
      "step": 429
    },
    {
      "epoch": 0.6259097525473072,
      "grad_norm": 0.39782917499542236,
      "learning_rate": 7.565982404692083e-05,
      "loss": 0.6429,
      "step": 430
    },
    {
      "epoch": 0.6273653566229985,
      "grad_norm": 0.43473222851753235,
      "learning_rate": 7.536656891495602e-05,
      "loss": 0.7088,
      "step": 431
    },
    {
      "epoch": 0.62882096069869,
      "grad_norm": 0.4322667717933655,
      "learning_rate": 7.507331378299121e-05,
      "loss": 0.8644,
      "step": 432
    },
    {
      "epoch": 0.6302765647743813,
      "grad_norm": 0.43406611680984497,
      "learning_rate": 7.478005865102639e-05,
      "loss": 0.7247,
      "step": 433
    },
    {
      "epoch": 0.6317321688500728,
      "grad_norm": 0.5200678706169128,
      "learning_rate": 7.448680351906158e-05,
      "loss": 0.6027,
      "step": 434
    },
    {
      "epoch": 0.6331877729257642,
      "grad_norm": 0.358857125043869,
      "learning_rate": 7.419354838709677e-05,
      "loss": 0.6444,
      "step": 435
    },
    {
      "epoch": 0.6346433770014556,
      "grad_norm": 0.41639384627342224,
      "learning_rate": 7.390029325513196e-05,
      "loss": 0.7388,
      "step": 436
    },
    {
      "epoch": 0.636098981077147,
      "grad_norm": 0.438455194234848,
      "learning_rate": 7.360703812316715e-05,
      "loss": 0.733,
      "step": 437
    },
    {
      "epoch": 0.6375545851528385,
      "grad_norm": 0.370872437953949,
      "learning_rate": 7.331378299120235e-05,
      "loss": 0.8471,
      "step": 438
    },
    {
      "epoch": 0.6390101892285298,
      "grad_norm": 0.44342750310897827,
      "learning_rate": 7.302052785923754e-05,
      "loss": 0.8589,
      "step": 439
    },
    {
      "epoch": 0.6404657933042213,
      "grad_norm": 0.47408348321914673,
      "learning_rate": 7.272727272727273e-05,
      "loss": 0.7668,
      "step": 440
    },
    {
      "epoch": 0.6419213973799127,
      "grad_norm": 0.43314129114151,
      "learning_rate": 7.243401759530792e-05,
      "loss": 0.6888,
      "step": 441
    },
    {
      "epoch": 0.6433770014556041,
      "grad_norm": 0.431505411863327,
      "learning_rate": 7.214076246334311e-05,
      "loss": 0.6909,
      "step": 442
    },
    {
      "epoch": 0.6448326055312955,
      "grad_norm": 0.3918209373950958,
      "learning_rate": 7.18475073313783e-05,
      "loss": 0.9539,
      "step": 443
    },
    {
      "epoch": 0.6462882096069869,
      "grad_norm": 0.44012776017189026,
      "learning_rate": 7.155425219941349e-05,
      "loss": 0.8383,
      "step": 444
    },
    {
      "epoch": 0.6477438136826783,
      "grad_norm": 0.36762312054634094,
      "learning_rate": 7.126099706744868e-05,
      "loss": 0.6965,
      "step": 445
    },
    {
      "epoch": 0.6491994177583698,
      "grad_norm": 0.3939236104488373,
      "learning_rate": 7.096774193548388e-05,
      "loss": 0.545,
      "step": 446
    },
    {
      "epoch": 0.6506550218340611,
      "grad_norm": 0.432131290435791,
      "learning_rate": 7.067448680351907e-05,
      "loss": 0.7427,
      "step": 447
    },
    {
      "epoch": 0.6521106259097526,
      "grad_norm": 0.4775153696537018,
      "learning_rate": 7.038123167155426e-05,
      "loss": 0.9723,
      "step": 448
    },
    {
      "epoch": 0.653566229985444,
      "grad_norm": 0.40706878900527954,
      "learning_rate": 7.008797653958945e-05,
      "loss": 0.6923,
      "step": 449
    },
    {
      "epoch": 0.6550218340611353,
      "grad_norm": 0.3823510706424713,
      "learning_rate": 6.979472140762464e-05,
      "loss": 0.7456,
      "step": 450
    },
    {
      "epoch": 0.6564774381368268,
      "grad_norm": 0.40238431096076965,
      "learning_rate": 6.950146627565983e-05,
      "loss": 0.8891,
      "step": 451
    },
    {
      "epoch": 0.6579330422125182,
      "grad_norm": 0.3726312518119812,
      "learning_rate": 6.920821114369502e-05,
      "loss": 0.7917,
      "step": 452
    },
    {
      "epoch": 0.6593886462882096,
      "grad_norm": 0.3783329725265503,
      "learning_rate": 6.891495601173021e-05,
      "loss": 0.6014,
      "step": 453
    },
    {
      "epoch": 0.660844250363901,
      "grad_norm": 0.3964117169380188,
      "learning_rate": 6.862170087976539e-05,
      "loss": 0.7234,
      "step": 454
    },
    {
      "epoch": 0.6622998544395924,
      "grad_norm": 0.41478458046913147,
      "learning_rate": 6.832844574780058e-05,
      "loss": 0.7892,
      "step": 455
    },
    {
      "epoch": 0.6637554585152838,
      "grad_norm": 0.439870685338974,
      "learning_rate": 6.803519061583577e-05,
      "loss": 0.79,
      "step": 456
    },
    {
      "epoch": 0.6652110625909753,
      "grad_norm": 0.43748369812965393,
      "learning_rate": 6.774193548387096e-05,
      "loss": 1.0729,
      "step": 457
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.4100598394870758,
      "learning_rate": 6.744868035190616e-05,
      "loss": 0.896,
      "step": 458
    },
    {
      "epoch": 0.6681222707423581,
      "grad_norm": 0.4462276101112366,
      "learning_rate": 6.715542521994135e-05,
      "loss": 0.7388,
      "step": 459
    },
    {
      "epoch": 0.6695778748180495,
      "grad_norm": 0.41512173414230347,
      "learning_rate": 6.686217008797654e-05,
      "loss": 1.0016,
      "step": 460
    },
    {
      "epoch": 0.6710334788937409,
      "grad_norm": 0.4283764660358429,
      "learning_rate": 6.656891495601173e-05,
      "loss": 0.9584,
      "step": 461
    },
    {
      "epoch": 0.6724890829694323,
      "grad_norm": 0.38697549700737,
      "learning_rate": 6.627565982404692e-05,
      "loss": 0.7983,
      "step": 462
    },
    {
      "epoch": 0.6739446870451238,
      "grad_norm": 0.3637562394142151,
      "learning_rate": 6.598240469208211e-05,
      "loss": 0.7465,
      "step": 463
    },
    {
      "epoch": 0.6754002911208151,
      "grad_norm": 0.38567596673965454,
      "learning_rate": 6.56891495601173e-05,
      "loss": 0.8624,
      "step": 464
    },
    {
      "epoch": 0.6768558951965066,
      "grad_norm": 0.3597548007965088,
      "learning_rate": 6.53958944281525e-05,
      "loss": 0.5303,
      "step": 465
    },
    {
      "epoch": 0.6783114992721979,
      "grad_norm": 0.42368289828300476,
      "learning_rate": 6.510263929618768e-05,
      "loss": 0.7392,
      "step": 466
    },
    {
      "epoch": 0.6797671033478894,
      "grad_norm": 0.39355653524398804,
      "learning_rate": 6.480938416422288e-05,
      "loss": 0.8017,
      "step": 467
    },
    {
      "epoch": 0.6812227074235808,
      "grad_norm": 0.4409653842449188,
      "learning_rate": 6.451612903225807e-05,
      "loss": 0.7801,
      "step": 468
    },
    {
      "epoch": 0.6826783114992722,
      "grad_norm": 0.4058488607406616,
      "learning_rate": 6.422287390029326e-05,
      "loss": 0.8348,
      "step": 469
    },
    {
      "epoch": 0.6841339155749636,
      "grad_norm": 0.44439753890037537,
      "learning_rate": 6.392961876832845e-05,
      "loss": 0.7076,
      "step": 470
    },
    {
      "epoch": 0.6855895196506551,
      "grad_norm": 0.4248671531677246,
      "learning_rate": 6.363636363636364e-05,
      "loss": 0.8604,
      "step": 471
    },
    {
      "epoch": 0.6870451237263464,
      "grad_norm": 0.4399133324623108,
      "learning_rate": 6.334310850439883e-05,
      "loss": 0.8135,
      "step": 472
    },
    {
      "epoch": 0.6885007278020379,
      "grad_norm": 0.6172164082527161,
      "learning_rate": 6.304985337243402e-05,
      "loss": 0.8129,
      "step": 473
    },
    {
      "epoch": 0.6899563318777293,
      "grad_norm": 0.45822566747665405,
      "learning_rate": 6.275659824046921e-05,
      "loss": 0.6153,
      "step": 474
    },
    {
      "epoch": 0.6914119359534207,
      "grad_norm": 0.43383681774139404,
      "learning_rate": 6.24633431085044e-05,
      "loss": 0.6966,
      "step": 475
    },
    {
      "epoch": 0.6928675400291121,
      "grad_norm": 0.3965972065925598,
      "learning_rate": 6.21700879765396e-05,
      "loss": 0.6607,
      "step": 476
    },
    {
      "epoch": 0.6943231441048034,
      "grad_norm": 0.5001950263977051,
      "learning_rate": 6.187683284457479e-05,
      "loss": 0.8804,
      "step": 477
    },
    {
      "epoch": 0.6957787481804949,
      "grad_norm": 0.4286115765571594,
      "learning_rate": 6.158357771260998e-05,
      "loss": 0.6095,
      "step": 478
    },
    {
      "epoch": 0.6972343522561864,
      "grad_norm": 0.37973302602767944,
      "learning_rate": 6.129032258064517e-05,
      "loss": 0.7778,
      "step": 479
    },
    {
      "epoch": 0.6986899563318777,
      "grad_norm": 0.3914148211479187,
      "learning_rate": 6.0997067448680354e-05,
      "loss": 0.8304,
      "step": 480
    },
    {
      "epoch": 0.7001455604075691,
      "grad_norm": 0.471505343914032,
      "learning_rate": 6.0703812316715545e-05,
      "loss": 0.7009,
      "step": 481
    },
    {
      "epoch": 0.7016011644832606,
      "grad_norm": 0.3576962649822235,
      "learning_rate": 6.0410557184750737e-05,
      "loss": 0.5475,
      "step": 482
    },
    {
      "epoch": 0.7030567685589519,
      "grad_norm": 0.3990599811077118,
      "learning_rate": 6.011730205278593e-05,
      "loss": 0.6439,
      "step": 483
    },
    {
      "epoch": 0.7045123726346434,
      "grad_norm": 0.38276562094688416,
      "learning_rate": 5.982404692082112e-05,
      "loss": 0.628,
      "step": 484
    },
    {
      "epoch": 0.7059679767103348,
      "grad_norm": 0.4065697491168976,
      "learning_rate": 5.953079178885631e-05,
      "loss": 0.8262,
      "step": 485
    },
    {
      "epoch": 0.7074235807860262,
      "grad_norm": 0.42395564913749695,
      "learning_rate": 5.92375366568915e-05,
      "loss": 0.7428,
      "step": 486
    },
    {
      "epoch": 0.7088791848617176,
      "grad_norm": 0.4335857629776001,
      "learning_rate": 5.894428152492669e-05,
      "loss": 0.7662,
      "step": 487
    },
    {
      "epoch": 0.710334788937409,
      "grad_norm": 0.4349001348018646,
      "learning_rate": 5.8651026392961884e-05,
      "loss": 1.0009,
      "step": 488
    },
    {
      "epoch": 0.7117903930131004,
      "grad_norm": 0.4048830568790436,
      "learning_rate": 5.8357771260997075e-05,
      "loss": 0.8147,
      "step": 489
    },
    {
      "epoch": 0.7132459970887919,
      "grad_norm": 0.38073739409446716,
      "learning_rate": 5.8064516129032266e-05,
      "loss": 0.6957,
      "step": 490
    },
    {
      "epoch": 0.7147016011644832,
      "grad_norm": 0.39902463555336,
      "learning_rate": 5.777126099706746e-05,
      "loss": 0.7158,
      "step": 491
    },
    {
      "epoch": 0.7161572052401747,
      "grad_norm": 0.3974488377571106,
      "learning_rate": 5.747800586510265e-05,
      "loss": 0.8136,
      "step": 492
    },
    {
      "epoch": 0.7176128093158661,
      "grad_norm": 0.408303827047348,
      "learning_rate": 5.718475073313784e-05,
      "loss": 0.8519,
      "step": 493
    },
    {
      "epoch": 0.7190684133915575,
      "grad_norm": 0.4230932295322418,
      "learning_rate": 5.689149560117303e-05,
      "loss": 0.9785,
      "step": 494
    },
    {
      "epoch": 0.7205240174672489,
      "grad_norm": 0.40179958939552307,
      "learning_rate": 5.659824046920822e-05,
      "loss": 0.7215,
      "step": 495
    },
    {
      "epoch": 0.7219796215429404,
      "grad_norm": 0.4078177213668823,
      "learning_rate": 5.630498533724341e-05,
      "loss": 0.8894,
      "step": 496
    },
    {
      "epoch": 0.7234352256186317,
      "grad_norm": 0.35702207684516907,
      "learning_rate": 5.601173020527859e-05,
      "loss": 0.8153,
      "step": 497
    },
    {
      "epoch": 0.7248908296943232,
      "grad_norm": 0.37978091835975647,
      "learning_rate": 5.571847507331378e-05,
      "loss": 0.9366,
      "step": 498
    },
    {
      "epoch": 0.7263464337700145,
      "grad_norm": 0.3947668671607971,
      "learning_rate": 5.542521994134897e-05,
      "loss": 0.8342,
      "step": 499
    },
    {
      "epoch": 0.727802037845706,
      "grad_norm": 0.40352654457092285,
      "learning_rate": 5.5131964809384164e-05,
      "loss": 0.8421,
      "step": 500
    },
    {
      "epoch": 0.7292576419213974,
      "grad_norm": 0.3716363310813904,
      "learning_rate": 5.4838709677419355e-05,
      "loss": 0.6389,
      "step": 501
    },
    {
      "epoch": 0.7307132459970888,
      "grad_norm": 0.3608885407447815,
      "learning_rate": 5.4545454545454546e-05,
      "loss": 0.6485,
      "step": 502
    },
    {
      "epoch": 0.7321688500727802,
      "grad_norm": 0.4496259391307831,
      "learning_rate": 5.425219941348974e-05,
      "loss": 1.0833,
      "step": 503
    },
    {
      "epoch": 0.7336244541484717,
      "grad_norm": 0.4333873987197876,
      "learning_rate": 5.395894428152493e-05,
      "loss": 0.9754,
      "step": 504
    },
    {
      "epoch": 0.735080058224163,
      "grad_norm": 0.4103349447250366,
      "learning_rate": 5.366568914956012e-05,
      "loss": 0.7551,
      "step": 505
    },
    {
      "epoch": 0.7365356622998545,
      "grad_norm": 0.4418387711048126,
      "learning_rate": 5.337243401759531e-05,
      "loss": 0.6815,
      "step": 506
    },
    {
      "epoch": 0.7379912663755459,
      "grad_norm": 0.37286463379859924,
      "learning_rate": 5.3079178885630495e-05,
      "loss": 0.7556,
      "step": 507
    },
    {
      "epoch": 0.7394468704512372,
      "grad_norm": 0.4646559953689575,
      "learning_rate": 5.2785923753665686e-05,
      "loss": 0.7566,
      "step": 508
    },
    {
      "epoch": 0.7409024745269287,
      "grad_norm": 0.38951393961906433,
      "learning_rate": 5.249266862170088e-05,
      "loss": 0.6457,
      "step": 509
    },
    {
      "epoch": 0.74235807860262,
      "grad_norm": 0.3815273940563202,
      "learning_rate": 5.219941348973607e-05,
      "loss": 0.887,
      "step": 510
    },
    {
      "epoch": 0.7438136826783115,
      "grad_norm": 0.406596839427948,
      "learning_rate": 5.190615835777126e-05,
      "loss": 0.724,
      "step": 511
    },
    {
      "epoch": 0.745269286754003,
      "grad_norm": 0.4192039966583252,
      "learning_rate": 5.161290322580645e-05,
      "loss": 0.7421,
      "step": 512
    },
    {
      "epoch": 0.7467248908296943,
      "grad_norm": 0.3701656460762024,
      "learning_rate": 5.131964809384164e-05,
      "loss": 0.8177,
      "step": 513
    },
    {
      "epoch": 0.7481804949053857,
      "grad_norm": 0.3817179501056671,
      "learning_rate": 5.102639296187683e-05,
      "loss": 0.7304,
      "step": 514
    },
    {
      "epoch": 0.7496360989810772,
      "grad_norm": 0.3974413573741913,
      "learning_rate": 5.0733137829912025e-05,
      "loss": 0.5927,
      "step": 515
    },
    {
      "epoch": 0.7510917030567685,
      "grad_norm": 0.4601061940193176,
      "learning_rate": 5.0439882697947216e-05,
      "loss": 0.9077,
      "step": 516
    },
    {
      "epoch": 0.75254730713246,
      "grad_norm": 0.3999941349029541,
      "learning_rate": 5.014662756598241e-05,
      "loss": 0.9505,
      "step": 517
    },
    {
      "epoch": 0.7540029112081513,
      "grad_norm": 0.4534831643104553,
      "learning_rate": 4.98533724340176e-05,
      "loss": 0.8408,
      "step": 518
    },
    {
      "epoch": 0.7554585152838428,
      "grad_norm": 0.43053725361824036,
      "learning_rate": 4.956011730205279e-05,
      "loss": 0.7802,
      "step": 519
    },
    {
      "epoch": 0.7569141193595342,
      "grad_norm": 0.38030388951301575,
      "learning_rate": 4.926686217008798e-05,
      "loss": 0.6853,
      "step": 520
    },
    {
      "epoch": 0.7583697234352256,
      "grad_norm": 0.4011993706226349,
      "learning_rate": 4.897360703812317e-05,
      "loss": 0.6086,
      "step": 521
    },
    {
      "epoch": 0.759825327510917,
      "grad_norm": 0.43298131227493286,
      "learning_rate": 4.868035190615836e-05,
      "loss": 0.7787,
      "step": 522
    },
    {
      "epoch": 0.7612809315866085,
      "grad_norm": 0.3767421841621399,
      "learning_rate": 4.8387096774193554e-05,
      "loss": 0.8143,
      "step": 523
    },
    {
      "epoch": 0.7627365356622998,
      "grad_norm": 0.3740949332714081,
      "learning_rate": 4.8093841642228745e-05,
      "loss": 0.7146,
      "step": 524
    },
    {
      "epoch": 0.7641921397379913,
      "grad_norm": 0.4165022671222687,
      "learning_rate": 4.7800586510263936e-05,
      "loss": 0.8808,
      "step": 525
    },
    {
      "epoch": 0.7656477438136827,
      "grad_norm": 0.4802544116973877,
      "learning_rate": 4.750733137829913e-05,
      "loss": 0.8033,
      "step": 526
    },
    {
      "epoch": 0.7671033478893741,
      "grad_norm": 0.3912263810634613,
      "learning_rate": 4.721407624633432e-05,
      "loss": 0.8414,
      "step": 527
    },
    {
      "epoch": 0.7685589519650655,
      "grad_norm": 0.38305243849754333,
      "learning_rate": 4.692082111436951e-05,
      "loss": 0.7193,
      "step": 528
    },
    {
      "epoch": 0.7700145560407569,
      "grad_norm": 0.4009001851081848,
      "learning_rate": 4.6627565982404694e-05,
      "loss": 0.828,
      "step": 529
    },
    {
      "epoch": 0.7714701601164483,
      "grad_norm": 0.42005327343940735,
      "learning_rate": 4.6334310850439885e-05,
      "loss": 0.6881,
      "step": 530
    },
    {
      "epoch": 0.7729257641921398,
      "grad_norm": 0.4526364803314209,
      "learning_rate": 4.6041055718475076e-05,
      "loss": 0.814,
      "step": 531
    },
    {
      "epoch": 0.7743813682678311,
      "grad_norm": 0.38356849551200867,
      "learning_rate": 4.574780058651027e-05,
      "loss": 0.4849,
      "step": 532
    },
    {
      "epoch": 0.7758369723435226,
      "grad_norm": 0.33577796816825867,
      "learning_rate": 4.545454545454546e-05,
      "loss": 0.5419,
      "step": 533
    },
    {
      "epoch": 0.777292576419214,
      "grad_norm": 0.4390701353549957,
      "learning_rate": 4.516129032258064e-05,
      "loss": 0.7791,
      "step": 534
    },
    {
      "epoch": 0.7787481804949054,
      "grad_norm": 0.41277047991752625,
      "learning_rate": 4.4868035190615834e-05,
      "loss": 0.6516,
      "step": 535
    },
    {
      "epoch": 0.7802037845705968,
      "grad_norm": 0.40979552268981934,
      "learning_rate": 4.4574780058651025e-05,
      "loss": 0.8655,
      "step": 536
    },
    {
      "epoch": 0.7816593886462883,
      "grad_norm": 0.40881484746932983,
      "learning_rate": 4.4281524926686216e-05,
      "loss": 0.8642,
      "step": 537
    },
    {
      "epoch": 0.7831149927219796,
      "grad_norm": 0.38998112082481384,
      "learning_rate": 4.398826979472141e-05,
      "loss": 0.7785,
      "step": 538
    },
    {
      "epoch": 0.784570596797671,
      "grad_norm": 0.37224289774894714,
      "learning_rate": 4.36950146627566e-05,
      "loss": 0.6966,
      "step": 539
    },
    {
      "epoch": 0.7860262008733624,
      "grad_norm": 0.40606194734573364,
      "learning_rate": 4.340175953079179e-05,
      "loss": 0.7882,
      "step": 540
    },
    {
      "epoch": 0.7874818049490538,
      "grad_norm": 0.39799171686172485,
      "learning_rate": 4.310850439882698e-05,
      "loss": 0.7723,
      "step": 541
    },
    {
      "epoch": 0.7889374090247453,
      "grad_norm": 0.43961989879608154,
      "learning_rate": 4.281524926686217e-05,
      "loss": 1.0004,
      "step": 542
    },
    {
      "epoch": 0.7903930131004366,
      "grad_norm": 0.41218143701553345,
      "learning_rate": 4.252199413489736e-05,
      "loss": 0.8822,
      "step": 543
    },
    {
      "epoch": 0.7918486171761281,
      "grad_norm": 0.40476685762405396,
      "learning_rate": 4.2228739002932555e-05,
      "loss": 0.9092,
      "step": 544
    },
    {
      "epoch": 0.7933042212518195,
      "grad_norm": 0.4009888172149658,
      "learning_rate": 4.1935483870967746e-05,
      "loss": 0.7668,
      "step": 545
    },
    {
      "epoch": 0.7947598253275109,
      "grad_norm": 0.3863876760005951,
      "learning_rate": 4.164222873900294e-05,
      "loss": 0.5909,
      "step": 546
    },
    {
      "epoch": 0.7962154294032023,
      "grad_norm": 0.5005965232849121,
      "learning_rate": 4.134897360703813e-05,
      "loss": 0.8557,
      "step": 547
    },
    {
      "epoch": 0.7976710334788938,
      "grad_norm": 0.41097232699394226,
      "learning_rate": 4.105571847507332e-05,
      "loss": 0.7503,
      "step": 548
    },
    {
      "epoch": 0.7991266375545851,
      "grad_norm": 0.42963793873786926,
      "learning_rate": 4.076246334310851e-05,
      "loss": 0.7932,
      "step": 549
    },
    {
      "epoch": 0.8005822416302766,
      "grad_norm": 0.390730619430542,
      "learning_rate": 4.0469208211143695e-05,
      "loss": 0.7395,
      "step": 550
    },
    {
      "epoch": 0.8020378457059679,
      "grad_norm": 0.37069982290267944,
      "learning_rate": 4.0175953079178886e-05,
      "loss": 0.7454,
      "step": 551
    },
    {
      "epoch": 0.8034934497816594,
      "grad_norm": 0.3857097029685974,
      "learning_rate": 3.988269794721408e-05,
      "loss": 0.7897,
      "step": 552
    },
    {
      "epoch": 0.8049490538573508,
      "grad_norm": 0.46153920888900757,
      "learning_rate": 3.958944281524927e-05,
      "loss": 0.8858,
      "step": 553
    },
    {
      "epoch": 0.8064046579330422,
      "grad_norm": 0.39734137058258057,
      "learning_rate": 3.929618768328446e-05,
      "loss": 0.8038,
      "step": 554
    },
    {
      "epoch": 0.8078602620087336,
      "grad_norm": 0.44264888763427734,
      "learning_rate": 3.900293255131965e-05,
      "loss": 1.0662,
      "step": 555
    },
    {
      "epoch": 0.8093158660844251,
      "grad_norm": 0.3777131736278534,
      "learning_rate": 3.870967741935484e-05,
      "loss": 0.4044,
      "step": 556
    },
    {
      "epoch": 0.8107714701601164,
      "grad_norm": 0.44934335350990295,
      "learning_rate": 3.841642228739003e-05,
      "loss": 0.7962,
      "step": 557
    },
    {
      "epoch": 0.8122270742358079,
      "grad_norm": 0.4084624648094177,
      "learning_rate": 3.8123167155425224e-05,
      "loss": 0.9061,
      "step": 558
    },
    {
      "epoch": 0.8136826783114993,
      "grad_norm": 0.4392997622489929,
      "learning_rate": 3.7829912023460415e-05,
      "loss": 0.7819,
      "step": 559
    },
    {
      "epoch": 0.8151382823871907,
      "grad_norm": 0.4539221525192261,
      "learning_rate": 3.7536656891495606e-05,
      "loss": 0.9805,
      "step": 560
    },
    {
      "epoch": 0.8165938864628821,
      "grad_norm": 0.39839059114456177,
      "learning_rate": 3.724340175953079e-05,
      "loss": 0.6616,
      "step": 561
    },
    {
      "epoch": 0.8180494905385735,
      "grad_norm": 0.42535200715065,
      "learning_rate": 3.695014662756598e-05,
      "loss": 0.7059,
      "step": 562
    },
    {
      "epoch": 0.8195050946142649,
      "grad_norm": 0.38165420293807983,
      "learning_rate": 3.665689149560117e-05,
      "loss": 0.8686,
      "step": 563
    },
    {
      "epoch": 0.8209606986899564,
      "grad_norm": 0.3542751669883728,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 0.6449,
      "step": 564
    },
    {
      "epoch": 0.8224163027656477,
      "grad_norm": 0.48850515484809875,
      "learning_rate": 3.6070381231671555e-05,
      "loss": 0.8892,
      "step": 565
    },
    {
      "epoch": 0.8238719068413392,
      "grad_norm": 0.4261418282985687,
      "learning_rate": 3.5777126099706746e-05,
      "loss": 0.8458,
      "step": 566
    },
    {
      "epoch": 0.8253275109170306,
      "grad_norm": 0.3887743651866913,
      "learning_rate": 3.548387096774194e-05,
      "loss": 0.4342,
      "step": 567
    },
    {
      "epoch": 0.826783114992722,
      "grad_norm": 0.44411760568618774,
      "learning_rate": 3.519061583577713e-05,
      "loss": 1.2122,
      "step": 568
    },
    {
      "epoch": 0.8282387190684134,
      "grad_norm": 0.37489691376686096,
      "learning_rate": 3.489736070381232e-05,
      "loss": 0.8965,
      "step": 569
    },
    {
      "epoch": 0.8296943231441049,
      "grad_norm": 0.3879382312297821,
      "learning_rate": 3.460410557184751e-05,
      "loss": 0.8231,
      "step": 570
    },
    {
      "epoch": 0.8311499272197962,
      "grad_norm": 0.41081592440605164,
      "learning_rate": 3.4310850439882695e-05,
      "loss": 0.7728,
      "step": 571
    },
    {
      "epoch": 0.8326055312954876,
      "grad_norm": 0.3874458074569702,
      "learning_rate": 3.401759530791789e-05,
      "loss": 0.7406,
      "step": 572
    },
    {
      "epoch": 0.834061135371179,
      "grad_norm": 0.40163490176200867,
      "learning_rate": 3.372434017595308e-05,
      "loss": 0.8708,
      "step": 573
    },
    {
      "epoch": 0.8355167394468704,
      "grad_norm": 0.4094494581222534,
      "learning_rate": 3.343108504398827e-05,
      "loss": 0.8328,
      "step": 574
    },
    {
      "epoch": 0.8369723435225619,
      "grad_norm": 0.39452773332595825,
      "learning_rate": 3.313782991202346e-05,
      "loss": 0.6394,
      "step": 575
    },
    {
      "epoch": 0.8384279475982532,
      "grad_norm": 0.38752493262290955,
      "learning_rate": 3.284457478005865e-05,
      "loss": 0.5996,
      "step": 576
    },
    {
      "epoch": 0.8398835516739447,
      "grad_norm": 0.404286652803421,
      "learning_rate": 3.255131964809384e-05,
      "loss": 1.027,
      "step": 577
    },
    {
      "epoch": 0.8413391557496361,
      "grad_norm": 0.4122052788734436,
      "learning_rate": 3.2258064516129034e-05,
      "loss": 0.7027,
      "step": 578
    },
    {
      "epoch": 0.8427947598253275,
      "grad_norm": 0.4811750054359436,
      "learning_rate": 3.1964809384164225e-05,
      "loss": 0.7809,
      "step": 579
    },
    {
      "epoch": 0.8442503639010189,
      "grad_norm": 0.4011289179325104,
      "learning_rate": 3.1671554252199416e-05,
      "loss": 0.6532,
      "step": 580
    },
    {
      "epoch": 0.8457059679767104,
      "grad_norm": 0.395601361989975,
      "learning_rate": 3.137829912023461e-05,
      "loss": 0.7675,
      "step": 581
    },
    {
      "epoch": 0.8471615720524017,
      "grad_norm": 0.4156775176525116,
      "learning_rate": 3.10850439882698e-05,
      "loss": 0.8222,
      "step": 582
    },
    {
      "epoch": 0.8486171761280932,
      "grad_norm": 0.41014888882637024,
      "learning_rate": 3.079178885630499e-05,
      "loss": 0.7443,
      "step": 583
    },
    {
      "epoch": 0.8500727802037845,
      "grad_norm": 0.37269118428230286,
      "learning_rate": 3.0498533724340177e-05,
      "loss": 0.7019,
      "step": 584
    },
    {
      "epoch": 0.851528384279476,
      "grad_norm": 0.4419962465763092,
      "learning_rate": 3.0205278592375368e-05,
      "loss": 0.8876,
      "step": 585
    },
    {
      "epoch": 0.8529839883551674,
      "grad_norm": 0.4368361234664917,
      "learning_rate": 2.991202346041056e-05,
      "loss": 0.9207,
      "step": 586
    },
    {
      "epoch": 0.8544395924308588,
      "grad_norm": 0.40444594621658325,
      "learning_rate": 2.961876832844575e-05,
      "loss": 0.6361,
      "step": 587
    },
    {
      "epoch": 0.8558951965065502,
      "grad_norm": 0.36840105056762695,
      "learning_rate": 2.9325513196480942e-05,
      "loss": 0.7964,
      "step": 588
    },
    {
      "epoch": 0.8573508005822417,
      "grad_norm": 0.3670385181903839,
      "learning_rate": 2.9032258064516133e-05,
      "loss": 0.5055,
      "step": 589
    },
    {
      "epoch": 0.858806404657933,
      "grad_norm": 0.4165569841861725,
      "learning_rate": 2.8739002932551324e-05,
      "loss": 0.7214,
      "step": 590
    },
    {
      "epoch": 0.8602620087336245,
      "grad_norm": 0.40940332412719727,
      "learning_rate": 2.8445747800586515e-05,
      "loss": 0.7387,
      "step": 591
    },
    {
      "epoch": 0.8617176128093159,
      "grad_norm": 0.36185717582702637,
      "learning_rate": 2.8152492668621706e-05,
      "loss": 0.7994,
      "step": 592
    },
    {
      "epoch": 0.8631732168850073,
      "grad_norm": 0.4215438663959503,
      "learning_rate": 2.785923753665689e-05,
      "loss": 0.8311,
      "step": 593
    },
    {
      "epoch": 0.8646288209606987,
      "grad_norm": 0.3834873139858246,
      "learning_rate": 2.7565982404692082e-05,
      "loss": 0.7149,
      "step": 594
    },
    {
      "epoch": 0.86608442503639,
      "grad_norm": 0.3939831554889679,
      "learning_rate": 2.7272727272727273e-05,
      "loss": 0.7759,
      "step": 595
    },
    {
      "epoch": 0.8675400291120815,
      "grad_norm": 0.4009106755256653,
      "learning_rate": 2.6979472140762464e-05,
      "loss": 0.7704,
      "step": 596
    },
    {
      "epoch": 0.868995633187773,
      "grad_norm": 0.4480924606323242,
      "learning_rate": 2.6686217008797655e-05,
      "loss": 0.86,
      "step": 597
    },
    {
      "epoch": 0.8704512372634643,
      "grad_norm": 0.4131304919719696,
      "learning_rate": 2.6392961876832843e-05,
      "loss": 0.9538,
      "step": 598
    },
    {
      "epoch": 0.8719068413391557,
      "grad_norm": 0.38113275170326233,
      "learning_rate": 2.6099706744868034e-05,
      "loss": 0.6065,
      "step": 599
    },
    {
      "epoch": 0.8733624454148472,
      "grad_norm": 0.43718385696411133,
      "learning_rate": 2.5806451612903226e-05,
      "loss": 0.8019,
      "step": 600
    },
    {
      "epoch": 0.8748180494905385,
      "grad_norm": 0.39741137623786926,
      "learning_rate": 2.5513196480938417e-05,
      "loss": 0.7986,
      "step": 601
    },
    {
      "epoch": 0.87627365356623,
      "grad_norm": 0.402018666267395,
      "learning_rate": 2.5219941348973608e-05,
      "loss": 0.8412,
      "step": 602
    },
    {
      "epoch": 0.8777292576419214,
      "grad_norm": 0.4534338712692261,
      "learning_rate": 2.49266862170088e-05,
      "loss": 0.8538,
      "step": 603
    },
    {
      "epoch": 0.8791848617176128,
      "grad_norm": 0.37566837668418884,
      "learning_rate": 2.463343108504399e-05,
      "loss": 0.798,
      "step": 604
    },
    {
      "epoch": 0.8806404657933042,
      "grad_norm": 0.4189794659614563,
      "learning_rate": 2.434017595307918e-05,
      "loss": 0.7327,
      "step": 605
    },
    {
      "epoch": 0.8820960698689956,
      "grad_norm": 0.41865408420562744,
      "learning_rate": 2.4046920821114372e-05,
      "loss": 0.703,
      "step": 606
    },
    {
      "epoch": 0.883551673944687,
      "grad_norm": 0.4078995883464813,
      "learning_rate": 2.3753665689149564e-05,
      "loss": 0.7492,
      "step": 607
    },
    {
      "epoch": 0.8850072780203785,
      "grad_norm": 0.4531792104244232,
      "learning_rate": 2.3460410557184755e-05,
      "loss": 0.8402,
      "step": 608
    },
    {
      "epoch": 0.8864628820960698,
      "grad_norm": 0.4019405245780945,
      "learning_rate": 2.3167155425219943e-05,
      "loss": 0.7103,
      "step": 609
    },
    {
      "epoch": 0.8879184861717613,
      "grad_norm": 0.4398720860481262,
      "learning_rate": 2.2873900293255134e-05,
      "loss": 0.7826,
      "step": 610
    },
    {
      "epoch": 0.8893740902474527,
      "grad_norm": 0.3884800970554352,
      "learning_rate": 2.258064516129032e-05,
      "loss": 0.8902,
      "step": 611
    },
    {
      "epoch": 0.8908296943231441,
      "grad_norm": 0.3666698932647705,
      "learning_rate": 2.2287390029325513e-05,
      "loss": 0.7074,
      "step": 612
    },
    {
      "epoch": 0.8922852983988355,
      "grad_norm": 0.42091646790504456,
      "learning_rate": 2.1994134897360704e-05,
      "loss": 0.921,
      "step": 613
    },
    {
      "epoch": 0.893740902474527,
      "grad_norm": 0.4753318428993225,
      "learning_rate": 2.1700879765395895e-05,
      "loss": 0.848,
      "step": 614
    },
    {
      "epoch": 0.8951965065502183,
      "grad_norm": 0.3865875005722046,
      "learning_rate": 2.1407624633431086e-05,
      "loss": 0.6928,
      "step": 615
    },
    {
      "epoch": 0.8966521106259098,
      "grad_norm": 0.38921254873275757,
      "learning_rate": 2.1114369501466277e-05,
      "loss": 0.7651,
      "step": 616
    },
    {
      "epoch": 0.8981077147016011,
      "grad_norm": 0.3724701404571533,
      "learning_rate": 2.082111436950147e-05,
      "loss": 0.5851,
      "step": 617
    },
    {
      "epoch": 0.8995633187772926,
      "grad_norm": 0.4376150369644165,
      "learning_rate": 2.052785923753666e-05,
      "loss": 0.6529,
      "step": 618
    },
    {
      "epoch": 0.901018922852984,
      "grad_norm": 0.4118236303329468,
      "learning_rate": 2.0234604105571847e-05,
      "loss": 0.8677,
      "step": 619
    },
    {
      "epoch": 0.9024745269286754,
      "grad_norm": 0.4380917549133301,
      "learning_rate": 1.994134897360704e-05,
      "loss": 0.5766,
      "step": 620
    },
    {
      "epoch": 0.9039301310043668,
      "grad_norm": 0.4472970962524414,
      "learning_rate": 1.964809384164223e-05,
      "loss": 0.9152,
      "step": 621
    },
    {
      "epoch": 0.9053857350800583,
      "grad_norm": 0.4289800524711609,
      "learning_rate": 1.935483870967742e-05,
      "loss": 1.0606,
      "step": 622
    },
    {
      "epoch": 0.9068413391557496,
      "grad_norm": 0.42631661891937256,
      "learning_rate": 1.9061583577712612e-05,
      "loss": 0.915,
      "step": 623
    },
    {
      "epoch": 0.9082969432314411,
      "grad_norm": 0.400543749332428,
      "learning_rate": 1.8768328445747803e-05,
      "loss": 0.6371,
      "step": 624
    },
    {
      "epoch": 0.9097525473071325,
      "grad_norm": 0.4254618287086487,
      "learning_rate": 1.847507331378299e-05,
      "loss": 0.8462,
      "step": 625
    },
    {
      "epoch": 0.9112081513828238,
      "grad_norm": 0.37723401188850403,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 0.7906,
      "step": 626
    },
    {
      "epoch": 0.9126637554585153,
      "grad_norm": 0.4215807318687439,
      "learning_rate": 1.7888563049853373e-05,
      "loss": 0.6841,
      "step": 627
    },
    {
      "epoch": 0.9141193595342066,
      "grad_norm": 0.4263688325881958,
      "learning_rate": 1.7595307917888564e-05,
      "loss": 0.8992,
      "step": 628
    },
    {
      "epoch": 0.9155749636098981,
      "grad_norm": 0.35846850275993347,
      "learning_rate": 1.7302052785923756e-05,
      "loss": 0.7471,
      "step": 629
    },
    {
      "epoch": 0.9170305676855895,
      "grad_norm": 0.4188619554042816,
      "learning_rate": 1.7008797653958943e-05,
      "loss": 0.9202,
      "step": 630
    },
    {
      "epoch": 0.9184861717612809,
      "grad_norm": 0.45432549715042114,
      "learning_rate": 1.6715542521994134e-05,
      "loss": 1.0559,
      "step": 631
    },
    {
      "epoch": 0.9199417758369723,
      "grad_norm": 0.41160598397254944,
      "learning_rate": 1.6422287390029326e-05,
      "loss": 0.6976,
      "step": 632
    },
    {
      "epoch": 0.9213973799126638,
      "grad_norm": 0.42466163635253906,
      "learning_rate": 1.6129032258064517e-05,
      "loss": 0.8312,
      "step": 633
    },
    {
      "epoch": 0.9228529839883551,
      "grad_norm": 0.3646198809146881,
      "learning_rate": 1.5835777126099708e-05,
      "loss": 0.6755,
      "step": 634
    },
    {
      "epoch": 0.9243085880640466,
      "grad_norm": 0.40323030948638916,
      "learning_rate": 1.55425219941349e-05,
      "loss": 0.6149,
      "step": 635
    },
    {
      "epoch": 0.925764192139738,
      "grad_norm": 0.39353203773498535,
      "learning_rate": 1.5249266862170089e-05,
      "loss": 0.8034,
      "step": 636
    },
    {
      "epoch": 0.9272197962154294,
      "grad_norm": 0.3815881013870239,
      "learning_rate": 1.495601173020528e-05,
      "loss": 0.6497,
      "step": 637
    },
    {
      "epoch": 0.9286754002911208,
      "grad_norm": 0.4203309714794159,
      "learning_rate": 1.4662756598240471e-05,
      "loss": 0.7898,
      "step": 638
    },
    {
      "epoch": 0.9301310043668122,
      "grad_norm": 0.40056681632995605,
      "learning_rate": 1.4369501466275662e-05,
      "loss": 0.6362,
      "step": 639
    },
    {
      "epoch": 0.9315866084425036,
      "grad_norm": 0.37740373611450195,
      "learning_rate": 1.4076246334310853e-05,
      "loss": 0.7849,
      "step": 640
    },
    {
      "epoch": 0.9330422125181951,
      "grad_norm": 0.40709662437438965,
      "learning_rate": 1.3782991202346041e-05,
      "loss": 0.5836,
      "step": 641
    },
    {
      "epoch": 0.9344978165938864,
      "grad_norm": 0.44734957814216614,
      "learning_rate": 1.3489736070381232e-05,
      "loss": 0.8143,
      "step": 642
    },
    {
      "epoch": 0.9359534206695779,
      "grad_norm": 0.39805498719215393,
      "learning_rate": 1.3196480938416422e-05,
      "loss": 0.8237,
      "step": 643
    },
    {
      "epoch": 0.9374090247452693,
      "grad_norm": 0.440177857875824,
      "learning_rate": 1.2903225806451613e-05,
      "loss": 0.652,
      "step": 644
    },
    {
      "epoch": 0.9388646288209607,
      "grad_norm": 0.3910229802131653,
      "learning_rate": 1.2609970674486804e-05,
      "loss": 0.6528,
      "step": 645
    },
    {
      "epoch": 0.9403202328966521,
      "grad_norm": 0.41352713108062744,
      "learning_rate": 1.2316715542521995e-05,
      "loss": 0.8636,
      "step": 646
    },
    {
      "epoch": 0.9417758369723436,
      "grad_norm": 0.3918810486793518,
      "learning_rate": 1.2023460410557186e-05,
      "loss": 0.7466,
      "step": 647
    },
    {
      "epoch": 0.9432314410480349,
      "grad_norm": 0.3978784680366516,
      "learning_rate": 1.1730205278592377e-05,
      "loss": 0.8271,
      "step": 648
    },
    {
      "epoch": 0.9446870451237264,
      "grad_norm": 0.43234503269195557,
      "learning_rate": 1.1436950146627567e-05,
      "loss": 0.8732,
      "step": 649
    },
    {
      "epoch": 0.9461426491994177,
      "grad_norm": 0.3411995470523834,
      "learning_rate": 1.1143695014662756e-05,
      "loss": 0.5919,
      "step": 650
    },
    {
      "epoch": 0.9475982532751092,
      "grad_norm": 0.40204060077667236,
      "learning_rate": 1.0850439882697947e-05,
      "loss": 0.7773,
      "step": 651
    },
    {
      "epoch": 0.9490538573508006,
      "grad_norm": 0.44684964418411255,
      "learning_rate": 1.0557184750733139e-05,
      "loss": 0.8986,
      "step": 652
    },
    {
      "epoch": 0.950509461426492,
      "grad_norm": 0.41870439052581787,
      "learning_rate": 1.026392961876833e-05,
      "loss": 0.7398,
      "step": 653
    },
    {
      "epoch": 0.9519650655021834,
      "grad_norm": 0.3972489833831787,
      "learning_rate": 9.97067448680352e-06,
      "loss": 0.7465,
      "step": 654
    },
    {
      "epoch": 0.9534206695778749,
      "grad_norm": 0.43008217215538025,
      "learning_rate": 9.67741935483871e-06,
      "loss": 0.7642,
      "step": 655
    },
    {
      "epoch": 0.9548762736535662,
      "grad_norm": 0.38994282484054565,
      "learning_rate": 9.384164222873902e-06,
      "loss": 0.7035,
      "step": 656
    },
    {
      "epoch": 0.9563318777292577,
      "grad_norm": 0.3519224524497986,
      "learning_rate": 9.090909090909091e-06,
      "loss": 0.8638,
      "step": 657
    },
    {
      "epoch": 0.9577874818049491,
      "grad_norm": 0.4926173686981201,
      "learning_rate": 8.797653958944282e-06,
      "loss": 0.835,
      "step": 658
    },
    {
      "epoch": 0.9592430858806404,
      "grad_norm": 0.3500979244709015,
      "learning_rate": 8.504398826979472e-06,
      "loss": 0.6112,
      "step": 659
    },
    {
      "epoch": 0.9606986899563319,
      "grad_norm": 0.4343133270740509,
      "learning_rate": 8.211143695014663e-06,
      "loss": 0.6363,
      "step": 660
    },
    {
      "epoch": 0.9621542940320232,
      "grad_norm": 0.42323610186576843,
      "learning_rate": 7.917888563049854e-06,
      "loss": 0.7753,
      "step": 661
    },
    {
      "epoch": 0.9636098981077147,
      "grad_norm": 0.42525234818458557,
      "learning_rate": 7.624633431085044e-06,
      "loss": 0.8949,
      "step": 662
    },
    {
      "epoch": 0.9650655021834061,
      "grad_norm": 0.4061911106109619,
      "learning_rate": 7.3313782991202354e-06,
      "loss": 0.6291,
      "step": 663
    },
    {
      "epoch": 0.9665211062590975,
      "grad_norm": 0.4340912997722626,
      "learning_rate": 7.038123167155427e-06,
      "loss": 0.9926,
      "step": 664
    },
    {
      "epoch": 0.9679767103347889,
      "grad_norm": 0.3967590630054474,
      "learning_rate": 6.744868035190616e-06,
      "loss": 0.8057,
      "step": 665
    },
    {
      "epoch": 0.9694323144104804,
      "grad_norm": 0.48275887966156006,
      "learning_rate": 6.451612903225806e-06,
      "loss": 0.8189,
      "step": 666
    },
    {
      "epoch": 0.9708879184861717,
      "grad_norm": 0.4685058295726776,
      "learning_rate": 6.1583577712609975e-06,
      "loss": 0.8094,
      "step": 667
    },
    {
      "epoch": 0.9723435225618632,
      "grad_norm": 0.49384185671806335,
      "learning_rate": 5.865102639296189e-06,
      "loss": 0.9691,
      "step": 668
    },
    {
      "epoch": 0.9737991266375546,
      "grad_norm": 0.3610553443431854,
      "learning_rate": 5.571847507331378e-06,
      "loss": 0.6327,
      "step": 669
    },
    {
      "epoch": 0.975254730713246,
      "grad_norm": 0.3911255896091461,
      "learning_rate": 5.278592375366569e-06,
      "loss": 0.7387,
      "step": 670
    },
    {
      "epoch": 0.9767103347889374,
      "grad_norm": 0.39431288838386536,
      "learning_rate": 4.98533724340176e-06,
      "loss": 0.7931,
      "step": 671
    },
    {
      "epoch": 0.9781659388646288,
      "grad_norm": 0.4503593444824219,
      "learning_rate": 4.692082111436951e-06,
      "loss": 1.0319,
      "step": 672
    },
    {
      "epoch": 0.9796215429403202,
      "grad_norm": 0.40810880064964294,
      "learning_rate": 4.398826979472141e-06,
      "loss": 0.8924,
      "step": 673
    },
    {
      "epoch": 0.9810771470160117,
      "grad_norm": 0.448564738035202,
      "learning_rate": 4.105571847507331e-06,
      "loss": 0.8996,
      "step": 674
    },
    {
      "epoch": 0.982532751091703,
      "grad_norm": 0.44985660910606384,
      "learning_rate": 3.812316715542522e-06,
      "loss": 0.8324,
      "step": 675
    },
    {
      "epoch": 0.9839883551673945,
      "grad_norm": 0.37408149242401123,
      "learning_rate": 3.5190615835777133e-06,
      "loss": 0.6749,
      "step": 676
    },
    {
      "epoch": 0.9854439592430859,
      "grad_norm": 0.47501301765441895,
      "learning_rate": 3.225806451612903e-06,
      "loss": 0.6873,
      "step": 677
    },
    {
      "epoch": 0.9868995633187773,
      "grad_norm": 0.36635079979896545,
      "learning_rate": 2.9325513196480943e-06,
      "loss": 0.5051,
      "step": 678
    },
    {
      "epoch": 0.9883551673944687,
      "grad_norm": 0.3903941512107849,
      "learning_rate": 2.6392961876832847e-06,
      "loss": 0.6499,
      "step": 679
    },
    {
      "epoch": 0.9898107714701602,
      "grad_norm": 0.37523409724235535,
      "learning_rate": 2.3460410557184754e-06,
      "loss": 0.8162,
      "step": 680
    },
    {
      "epoch": 0.9912663755458515,
      "grad_norm": 0.4106103181838989,
      "learning_rate": 2.0527859237536657e-06,
      "loss": 0.9686,
      "step": 681
    },
    {
      "epoch": 0.992721979621543,
      "grad_norm": 0.40206608176231384,
      "learning_rate": 1.7595307917888567e-06,
      "loss": 0.7326,
      "step": 682
    },
    {
      "epoch": 0.9941775836972343,
      "grad_norm": 0.43989768624305725,
      "learning_rate": 1.4662756598240472e-06,
      "loss": 0.5903,
      "step": 683
    },
    {
      "epoch": 0.9956331877729258,
      "grad_norm": 0.42767128348350525,
      "learning_rate": 1.1730205278592377e-06,
      "loss": 1.0366,
      "step": 684
    },
    {
      "epoch": 0.9970887918486172,
      "grad_norm": 0.3868125379085541,
      "learning_rate": 8.797653958944283e-07,
      "loss": 0.6837,
      "step": 685
    },
    {
      "epoch": 0.9985443959243085,
      "grad_norm": 0.41021212935447693,
      "learning_rate": 5.865102639296188e-07,
      "loss": 0.7904,
      "step": 686
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.4045282304286957,
      "learning_rate": 2.932551319648094e-07,
      "loss": 0.9486,
      "step": 687
    }
  ],
  "logging_steps": 1,
  "max_steps": 687,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2840241762877440.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
